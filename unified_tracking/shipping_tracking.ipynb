{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Post Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Object': [{'JobId': 2293867,\n",
       "   'StatusHistory': [{'Status': 'Incomplete',\n",
       "     'Date': '2025-08-12T09:43:43.84'},\n",
       "    {'Status': 'Business Despatched', 'Date': '2025-08-12T09:44:02.187'},\n",
       "    {'Status': 'Business Despatched', 'Date': '2025-08-12T09:43:58.31'},\n",
       "    {'Status': 'Business Despatched', 'Date': '2025-08-12T09:45:12'},\n",
       "    {'Status': 'Business In Transit', 'Date': '2025-08-12T12:30:19'},\n",
       "    {'Status': 'Business In Transit', 'Date': '2025-08-12T17:13:36'},\n",
       "    {'Status': 'Business In Transit', 'Date': '2025-08-14T05:54:34'},\n",
       "    {'Status': 'Business In Transit', 'Date': '2025-08-15T07:28:17'},\n",
       "    {'Status': 'Business On Delivery', 'Date': '2025-08-15T07:48:41'},\n",
       "    {'Status': 'Business Delivered', 'Date': '2025-08-15T12:04:42'}]}],\n",
       " 'Errors': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "access_token = \"26lyJ90vNka0wPOJtmPcbw\"\n",
    "\n",
    "def get_bigpost_tracking(job_ids, access_token, base_url=\"https://api.bigpost.com.au\", timeout=25):\n",
    "    \"\"\"\n",
    "    POST /api/gettracking to fetch status history for one or more jobs.\n",
    "\n",
    "    Args:\n",
    "        job_ids (list[int|str]): Job IDs\n",
    "        access_token (str): BigPost API AccessToken\n",
    "        base_url (str): API base\n",
    "        timeout (int|float): request timeout seconds\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed JSON from the API\n",
    "    \"\"\"\n",
    "    url = f\"{base_url.rstrip('/')}/api/gettracking\"\n",
    "    headers = {\n",
    "        \"AccessToken\": access_token,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    # API examples show strings. Coerce to strings to be safe.\n",
    "    payload = [str(j) for j in job_ids]\n",
    "\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "sample_job_ids = [\n",
    "    2293867, 2293867\n",
    "]\n",
    "\n",
    "access_token = \"26lyJ90vNka0wPOJtmPcbw\"\n",
    "\n",
    "\n",
    "# Run tracking\n",
    "tracking_results = get_bigpost_tracking(sample_job_ids, access_token)\n",
    "\n",
    "tracking_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cario Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Object': [{'JobId': 9508079,\n",
       "   'StatusHistory': [{'Status': 'Shipment notification sent to k....@gmail.com',\n",
       "     'Date': '2025-08-11T15:47:00+00:00'},\n",
       "    {'Status': 'Ready for pickup', 'Date': '2025-08-11T16:02:00+00:00'},\n",
       "    {'Status': 'Collected from Sender', 'Date': '2025-08-12T14:40:00+00:00'},\n",
       "    {'Status': 'In Transit', 'Date': '2025-08-12T17:12:00+00:00'},\n",
       "    {'Status': 'Shipment Created BIG-62620',\n",
       "     'Date': '2025-08-13T00:00:00+00:00'},\n",
       "    {'Status': 'In Transit', 'Date': '2025-08-13T10:01:00+00:00'},\n",
       "    {'Status': 'POD', 'Date': '2025-08-14T01:52:00+00:00'},\n",
       "    {'Status': 'On Board for Delivery', 'Date': '2025-08-14T11:04:00+00:00'},\n",
       "    {'Status': 'Delivered', 'Date': '2025-08-14T11:41:00+00:00'}]}],\n",
       " 'Errors': [{'memberNames': ['8879850005090'],\n",
       "   'errorMessage': '404 Client Error: Not Found for url: https://integrate.cario.com.au/api//ConsignmentStatus/Query/8879850005090',\n",
       "   'validationType': None}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "    \n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Define the base URL for the API\n",
    "BASE_URL = \"https://integrate.cario.com.au/api/\"\n",
    "CUSTOMER_ID = 7115\n",
    "COUNTRY_ID = 36\n",
    "TENANT_ID = \"\"\n",
    "\n",
    "AUTH_TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJGTVNUb2tlbiI6ImV5SmhiR2NpT2lKSVV6STFOaUlzSW5SNWNDSTZJa3BYVkNKOS5leUpvZEhSd09pOHZjMk5vWlcxaGN5NTRiV3h6YjJGd0xtOXlaeTkzY3k4eU1EQTFMekExTDJsa1pXNTBhWFI1TDJOc1lXbHRjeTl1WVcxbGFXUmxiblJwWm1sbGNpSTZJakV6TWpVd0lpd2lhSFIwY0RvdkwzTmphR1Z0WVhNdWVHMXNjMjloY0M1dmNtY3ZkM012TWpBd05TOHdOUzlwWkdWdWRHbDBlUzlqYkdGcGJYTXZibUZ0WlNJNklrTjFjM1J2YlVCcGJuUmxaM0poZEdVM01URTFJaXdpYUhSMGNEb3ZMM05qYUdWdFlYTXVlRzFzYzI5aGNDNXZjbWN2ZDNNdk1qQXdOUzh3TlM5cFpHVnVkR2wwZVM5amJHRnBiWE12WlcxaGFXeGhaR1J5WlhOeklqb2lRM1Z6ZEc5dFFHbHVkR1ZuY21GMFpUY3hNVFV1YVdRaUxDSkJjM0JPWlhRdVNXUmxiblJwZEhrdVUyVmpkWEpwZEhsVGRHRnRjQ0k2SWtoSlNUSk5TakpYUTFCTlJrNUNObFZIUVROSVNWSTBRazlhTmpOSU5rZElJaXdpYUhSMGNEb3ZMM2QzZHk1aGMzQnVaWFJpYjJsc1pYSndiR0YwWlM1amIyMHZhV1JsYm5ScGRIa3ZZMnhoYVcxekwzUmxibUZ1ZEVsa0lqb2lPU0lzSWtOMWMzUnZiV1Z5U1VRaU9pSTNNVEUxSWl3aVFYQndiR2xqWVhScGIyNU9ZVzFsSWpvaVEzVnpkRzl0SWl3aWMzVmlJam9pTVRNeU5UQWlMQ0pxZEdraU9pSTBPRE15T1dReU1pMDFORGhpTFRReE5UTXRPVGt3WlMwd01EY3dOVE15TnpnNVpUa2lMQ0pwWVhRaU9qRTNNamszTlRNd05UZ3NJbWx6Y3lJNklrWk5VeUlzSW1GMVpDSTZJa1pOVXlKOS5LSEFoaUpRMDU5Qk5UNWhMYnNNNTVoZF94RjdWbmw4UWJfSDM1bEFuQ09nIiwibmJmIjoxNzI5NzUzMDU4LCJleHAiOjE3NjEyODkwNTgsImlzcyI6ImludGVncmF0ZS5jYXJpby5jb20uYXUiLCJhdWQiOiJpbnRlZ3JhdGVkY2FyaW9jdXN0b21lcnMifQ.E-KuuetkhAWtF-dm-1w133n82kq93svCXnhXKezQYio\"\n",
    "\n",
    "AUTH_TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1laWRlbnRpZmllciI6Ijk5MDkiLCJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1lIjoiWl9HcmlsbHNfQXVzdHJhbGlhIiwiaHR0cDovL3NjaGVtYXMueG1sc29hcC5vcmcvd3MvMjAwNS8wNS9pZGVudGl0eS9jbGFpbXMvZW1haWxhZGRyZXNzIjoianVzdGluLm5nQHpncmlsbHMuY29tLmF1IiwiQXNwTmV0LklkZW50aXR5LlNlY3VyaXR5U3RhbXAiOiJNNVBVUFVZSUhYSVBYQ0ZSNVBDVjJWWVNMRFVUNFQ2QSIsImh0dHA6Ly9zY2hlbWFzLm1pY3Jvc29mdC5jb20vd3MvMjAwOC8wNi9pZGVudGl0eS9jbGFpbXMvcm9sZSI6IkRlc3BhdGNoUGFyZW50QWNjdCIsImh0dHA6Ly93d3cuYXNwbmV0Ym9pbGVycGxhdGUuY29tL2lkZW50aXR5L2NsYWltcy90ZW5hbnRJZCI6IjkiLCJDdXN0b21lcklEIjoiNzExNSIsInN1YiI6Ijk5MDkiLCJqdGkiOiIyZTlhMWIzMi1kZDg1LTRkM2UtOTZiYS1iMmY5ZWExMTk1YWMiLCJpYXQiOjE3NTYzNjExMTIsIm5iZiI6MTc1NjM2MTExMiwiZXhwIjoxNzU2NDQ3NTEyLCJpc3MiOiJGTVMiLCJhdWQiOiJGTVMifQ.gpX61fJ9CCpYX_BQFj5SDhJam9hV_1w-dRvCyayOJuk\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
    "    \"CustomerId\": str(CUSTOMER_ID),\n",
    "    \"TenantId\": TENANT_ID,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_tracking_cario(consignment_numbers):\n",
    "    \"\"\"\n",
    "    Fetch tracking info from Cario for a list of consignments.\n",
    "\n",
    "    Args:\n",
    "        consignment_numbers (list[str|int]): Consignment numbers to query.\n",
    "\n",
    "    Returns:\n",
    "        dict: consignment_number -> tracking JSON\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for cn in consignment_numbers:\n",
    "        url = f\"{BASE_URL}/ConsignmentStatus/Query/{cn}\"\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            print(resp.status_code)\n",
    "            resp.raise_for_status()\n",
    "            results[str(cn)] = resp.json()\n",
    "        except Exception as e:\n",
    "            results[str(cn)] = {\"error\": str(e)}\n",
    "    return results\n",
    "\n",
    "def _safe_parse_iso(ts: Optional[str]) -> Optional[datetime]:\n",
    "    if not ts:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def format_cario_events(results_by_consignment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input: results_by_consignment is your {consignment_number: API JSON or {'error': ...}}\n",
    "    Output: {'Object': [{'JobId': <id>, 'StatusHistory': [{'Status': str, 'Date': str}, ...]}, ...],\n",
    "             'Errors': [{'memberNames':[<consignment>], 'errorMessage': str, 'validationType': None}, ...] or None}\n",
    "    \"\"\"\n",
    "    objects: List[Dict[str, Any]] = []\n",
    "    errors: List[Dict[str, Any]] = []\n",
    "\n",
    "    for consignment, payload in results_by_consignment.items():\n",
    "        # Error from fetch layer\n",
    "        if isinstance(payload, dict) and \"error\" in payload and \"events\" not in payload:\n",
    "            errors.append({\n",
    "                \"memberNames\": [str(consignment)],\n",
    "                \"errorMessage\": str(payload.get(\"error\")),\n",
    "                \"validationType\": None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        data = payload or {}\n",
    "        # Handle case where payload is a list (multiple records per consignment)\n",
    "        if isinstance(data, list):\n",
    "            if len(data) > 0:\n",
    "                # Take the first record from the list\n",
    "                data = data[0]\n",
    "            else:\n",
    "                # Empty list, skip this consignment\n",
    "                continue\n",
    "        \n",
    "        job_id = data.get(\"id\") or data.get(\"consignmentId\") or data.get(\"ID\") or consignment\n",
    "\n",
    "        # Pull events list\n",
    "        events = data.get(\"events\") or data.get(\"statusHistory\") or []\n",
    "\n",
    "        # Map to desired shape with fallbacks\n",
    "        mapped = []\n",
    "        seen = set()\n",
    "        for ev in events:\n",
    "            status = ev.get(\"milestone\") or ev.get(\"comments\") or ev.get(\"status\") or ev.get(\"eventType\") or \"\"\n",
    "            date = ev.get(\"eventTime\") or ev.get(\"date\") or ev.get(\"event_date\") or None\n",
    "            key = (status, date)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            mapped.append({\"Status\": status, \"Date\": date})\n",
    "\n",
    "        # Sort by Date ascending where possible\n",
    "        mapped.sort(key=lambda x: (_safe_parse_iso(x[\"Date\"]) or datetime.max, x[\"Date\"] or \"\"))\n",
    "\n",
    "        objects.append({\n",
    "            \"JobId\": int(job_id) if str(job_id).isdigit() else job_id,\n",
    "            \"StatusHistory\": mapped\n",
    "        })\n",
    "\n",
    "    return {\"Object\": objects, \"Errors\": errors or None}\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_cons = [\n",
    "    \"8879850005089\",\n",
    "    \"8879850005089\",\n",
    "    \"8879850005090\"\n",
    "]\n",
    "\n",
    "raw_tracking_data = get_tracking_cario(sample_cons)\n",
    "tracking_data = format_cario_events(raw_tracking_data)\n",
    "tracking_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFS Macship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Object\": [\n",
      "    {\n",
      "      \"JobId\": 52768539,\n",
      "      \"StatusHistory\": [\n",
      "        {\n",
      "          \"MachshipStatus\": \"Unmanifested\",\n",
      "          \"CarrierStatus\": null,\n",
      "          \"DateUtc\": \"2025-08-22T13:37:03.46\",\n",
      "          \"Info\": null\n",
      "        },\n",
      "        {\n",
      "          \"MachshipStatus\": \"Complete\",\n",
      "          \"CarrierStatus\": \"POD Received\",\n",
      "          \"DateUtc\": \"2025-08-28T06:00:29.01\",\n",
      "          \"Info\": null\n",
      "        },\n",
      "        {\n",
      "          \"MachshipStatus\": \"Manifested\",\n",
      "          \"CarrierStatus\": null,\n",
      "          \"DateUtc\": \"2025-08-22T13:37:04.107\",\n",
      "          \"Info\": null\n",
      "        },\n",
      "        {\n",
      "          \"MachshipStatus\": \"Booked\",\n",
      "          \"CarrierStatus\": \"Waiting for pickup\",\n",
      "          \"DateUtc\": \"2025-08-22T13:40:53.827\",\n",
      "          \"Info\": null\n",
      "        },\n",
      "        {\n",
      "          \"MachshipStatus\": \"Picked Up\",\n",
      "          \"CarrierStatus\": \"Picked up\",\n",
      "          \"DateUtc\": \"2025-08-28T03:46:00\",\n",
      "          \"Info\": null\n",
      "        },\n",
      "        {\n",
      "          \"MachshipStatus\": \"On For Delivery\",\n",
      "          \"CarrierStatus\": \"On board for delivery\",\n",
      "          \"DateUtc\": \"2025-08-28T04:10:33.347\",\n",
      "          \"Info\": null\n",
      "        },\n",
      "        {\n",
      "          \"MachshipStatus\": \"Complete\",\n",
      "          \"CarrierStatus\": \"Delivered\",\n",
      "          \"DateUtc\": \"2025-08-28T05:46:00\",\n",
      "          \"Info\": \"https://appsrv.bondscouriers.com.au/jobphotos/4/4940/4940554_2_20250828_154934.jpg\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"Errors\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "BASE_URL = \"https://live.machship.com\"\n",
    "API_TOKEN = \"MkfE0KS0GU6_RG4UC5P1bQfnmF_M9HB02uj_PpuwtpCQ\"\n",
    "\n",
    "def _headers(token: str) -> Dict[str, str]:\n",
    "    \"\"\"Generates standard request headers.\"\"\"\n",
    "    return {\n",
    "        \"token\": token,\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "def _safe_parse_iso(ts: Optional[str]) -> Optional[datetime]:\n",
    "    \"\"\"Safely parses an ISO 8601 timestamp string into a datetime object.\"\"\"\n",
    "    if not ts: return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def get_machship_statuses(\n",
    "    ms_ids: List[int],\n",
    "    *,\n",
    "    token: str = API_TOKEN,\n",
    "    base_url: str = BASE_URL,\n",
    "    timeout: int = 30,\n",
    "    batch_size: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls POST /apiv2/consignments/returnConsignmentStatuses and correctly\n",
    "    parses the detailed tracking update response.\n",
    "    \"\"\"\n",
    "    if not ms_ids:\n",
    "        return {\"Object\": [], \"Errors\": None}\n",
    "\n",
    "    url = f\"{base_url.rstrip('/')}/apiv2/consignments/returnConsignmentStatuses\"\n",
    "    \n",
    "    objects: List[Dict[str, Any]] = []\n",
    "    errors: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(0, len(ms_ids), batch_size):\n",
    "        chunk = [int(x) for x in ms_ids[i:i + batch_size] if str(x).strip().isdigit()]\n",
    "        if not chunk: continue\n",
    "\n",
    "        payload = {\"ids\": chunk}\n",
    "        try:\n",
    "            resp = requests.post(url, headers=_headers(token), json=payload, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            errors.append({\"errorMessage\": f\"Request failed: {e}\"})\n",
    "            continue\n",
    "        \n",
    "        status_histories = data.get(\"object\", [])\n",
    "        for history in status_histories:\n",
    "            job_id = history.get(\"consignmentId\")\n",
    "            \n",
    "            # ✅ CORRECTED PARSING LOGIC HERE\n",
    "            # This now correctly navigates the nested JSON structure.\n",
    "            parsed_statuses = []\n",
    "            for s in history.get(\"statusHistory\", []):\n",
    "                status_obj = s.get(\"consignmentTrackingStatus\", {})\n",
    "                parsed_statuses.append({\n",
    "                    \"MachshipStatus\": status_obj.get(\"name\"),\n",
    "                    \"CarrierStatus\": s.get(\"carrierStatusDescription\"),\n",
    "                    \"DateUtc\": s.get(\"statusDateUtc\"),\n",
    "                    \"Info\": s.get(\"statusInformation\")\n",
    "                })\n",
    "\n",
    "            # Sort by date, oldest first\n",
    "            parsed_statuses.sort(key=lambda x: (_safe_parse_iso(x[\"DateUtc\"]) or datetime.min))\n",
    "            \n",
    "            objects.append({\n",
    "                \"JobId\": job_id,\n",
    "                \"StatusHistory\": parsed_statuses,\n",
    "            })\n",
    "            \n",
    "        if data.get(\"errors\"):\n",
    "            errors.extend(data[\"errors\"])\n",
    "\n",
    "    return {\"Object\": objects, \"Errors\": errors or None}\n",
    "\n",
    "# --- Example Usage ---\n",
    "ms_codes_to_test = [\"52768539\"]\n",
    "out = get_machship_statuses(ms_codes_to_test)\n",
    "\n",
    "print(json.dumps(out, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing FINAL CORRECTED Unified Tracking ===\n",
      "Fetching data for 1 jobs from BigPost...\n",
      "Fetching data for 1 jobs from Cario...\n",
      "Cario URL: https://integrate.cario.com.au/api//ConsignmentStatus/Query/8879850005089\n",
      "Cario API for 8879850005089: Status 200\n",
      "Fetching data for 1 jobs from MachShip...\n",
      "\n",
      "--- ✅ FINAL CORRECTED Enriched Tracking DataFrame ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>customer_order_ref</th>\n",
       "      <th>latest_status</th>\n",
       "      <th>latest_update_utc</th>\n",
       "      <th>latest_details</th>\n",
       "      <th>full_tracking_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52768539</td>\n",
       "      <td>MachShip</td>\n",
       "      <td>ORD-1001</td>\n",
       "      <td>DELIVERED</td>\n",
       "      <td>2025-08-28 05:46:00+00:00</td>\n",
       "      <td>Machship: Complete / Carrier: Delivered</td>\n",
       "      <td>[{'timestamp_utc': None, 'standard_status': 'BOOKED', 'details': 'Machship: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8879850005089</td>\n",
       "      <td>Cario</td>\n",
       "      <td>ORD-1002</td>\n",
       "      <td>DELIVERED</td>\n",
       "      <td>2025-08-14 11:41:00+00:00</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>[{'timestamp_utc': 2025-08-11 15:47:00+00:00, 'standard_status': 'UNKNOWN', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293867</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>ORD-1003</td>\n",
       "      <td>DELIVERED</td>\n",
       "      <td>2025-08-15 12:04:42+00:00</td>\n",
       "      <td>Business Delivered</td>\n",
       "      <td>[{'timestamp_utc': None, 'standard_status': 'DELIVERED', 'details': 'Incompl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          job_id  platform customer_order_ref latest_status  \\\n",
       "0       52768539  MachShip           ORD-1001     DELIVERED   \n",
       "1  8879850005089     Cario           ORD-1002     DELIVERED   \n",
       "2        2293867   BigPost           ORD-1003     DELIVERED   \n",
       "\n",
       "          latest_update_utc                           latest_details  \\\n",
       "0 2025-08-28 05:46:00+00:00  Machship: Complete / Carrier: Delivered   \n",
       "1 2025-08-14 11:41:00+00:00                                Delivered   \n",
       "2 2025-08-15 12:04:42+00:00                       Business Delivered   \n",
       "\n",
       "                                                             full_tracking_history  \n",
       "0  [{'timestamp_utc': None, 'standard_status': 'BOOKED', 'details': 'Machship: ...  \n",
       "1  [{'timestamp_utc': 2025-08-11 15:47:00+00:00, 'standard_status': 'UNKNOWN', ...  \n",
       "2  [{'timestamp_utc': None, 'standard_status': 'DELIVERED', 'details': 'Incompl...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  FINAL CORRECTED Unified Freight Tracking Hub \n",
    "# ==============================================================================\n",
    "#\n",
    "#  Issues fixed:\n",
    "#  1. BigPost API returns dict with \"Object\" key, not a list directly - FIXED\n",
    "#  2. Missing TenantId header for Cario - FIXED  \n",
    "#  3. BigPost transformation expects list but gets dict - FIXED\n",
    "#  4. Cario URL double slash issue - FIXED\n",
    "#  5. Proper error handling and logging - ADDED\n",
    "#  6. Timezone-aware datetime conversion error - FIXED\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# --- ⚙️ Credentials & Configuration (Updated) ---\n",
    "CREDENTIALS_FINAL = {\n",
    "    \"MachShip\": {\n",
    "        \"api_token\": \"MkfE0KS0GU6_RG4UC5P1bQfnmF_M9HB02uj_PpuwtpCQ\",\n",
    "        \"base_url\": \"https://live.machship.com\"\n",
    "    },\n",
    "    \"Cario\": {\n",
    "        \"auth_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1laWRlbnRpZmllciI6Ijk5MDkiLCJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1lIjoiWl9HcmlsbHNfQXVzdHJhbGlhIiwiaHR0cDovL3NjaGVtYXMueG1sc29hcC5vcmcvd3MvMjAwNS8wNS9pZGVudGl0eS9jbGFpbXMvZW1haWxhZGRyZXNzIjoianVzdGluLm5nQHpncmlsbHMuY29tLmF1IiwiQXNwTmV0LklkZW50aXR5LlNlY3VyaXR5U3RhbXAiOiJNNVBVUFVZSUhYSVBYQ0ZSNVBDVjJWWVNMRFVUNFQ2QSIsImh0dHA6Ly9zY2hlbWFzLm1pY3Jvc29mdC5jb20vd3MvMjAwOC8wNi9pZGVudGl0eS9jbGFpbXMvcm9sZSI6IkRlc3BhdGNoUGFyZW50QWNjdCIsImh0dHA6Ly93d3cuYXNwbmV0Ym9pbGVycGxhdGUuY29tL2lkZW50aXR5L2NsYWltcy90ZW5hbnRJZCI6IjkiLCJDdXN0b21lcklEIjoiNzExNSIsInN1YiI6Ijk5MDkiLCJqdGkiOiIyZTlhMWIzMi1kZDg1LTRkM2UtOTZiYS1iMmY5ZWExMTk1YWMiLCJpYXQiOjE3NTYzNjExMTIsIm5iZiI6MTc1NjM2MTExMiwiZXhwIjoxNzU2NDQ3NTEyLCJpc3MiOiJGTVMiLCJhdWQiOiJGTVMifQ.gpX61fJ9CCpYX_BQFj5SDhJam9hV_1w-dRvCyayOJuk\",\n",
    "        \"customer_id\": \"7115\",\n",
    "        \"tenant_id\": \"\",  # ADDED: This was missing\n",
    "        \"base_url\": \"https://integrate.cario.com.au/api/\"  # Keep trailing slash for double slash\n",
    "    },\n",
    "    \"BigPost\": {\n",
    "        \"access_token\": \"26lyJ90vNka0wPOJtmPcbw\",\n",
    "        \"base_url\": \"https://api.bigpost.com.au\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# --- 🌐 Layer 1: API Connectors (CORRECTED) ---\n",
    "def _get_machship_tracking_raw_final(job_ids: List[str], creds: Dict) -> Dict:\n",
    "    url = f\"{creds['base_url'].rstrip('/')}/apiv2/consignments/returnConsignmentStatuses\"\n",
    "    headers = {\"token\": creds['api_token'], \"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json={\"ids\": [int(j) for j in job_ids]}, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"MachShip API Error: {e}\")\n",
    "        return {\"errors\": [{\"errorMessage\": str(e)}]}\n",
    "\n",
    "def _get_cario_tracking_raw_final(job_ids: List[str], creds: Dict) -> Dict:\n",
    "    results = {}\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {creds['auth_token']}\", \n",
    "        \"CustomerId\": creds['customer_id'], \n",
    "        \"TenantId\": creds.get('tenant_id', ''),  # FIXED: Added missing TenantId header\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    for job_id in job_ids:\n",
    "        # FIXED: Keep the double slash pattern like your working code\n",
    "        url = f\"{creds['base_url']}/ConsignmentStatus/Query/{job_id}\"\n",
    "        print(f\"Cario URL: {url}\")  # Debug output\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=30)\n",
    "            print(f\"Cario API for {job_id}: Status {resp.status_code}\")\n",
    "            if resp.ok:\n",
    "                results[str(job_id)] = resp.json()\n",
    "            else:\n",
    "                print(f\"Cario API Error for {job_id}: {resp.status_code}\")\n",
    "                results[str(job_id)] = {\"error\": f\"HTTP {resp.status_code}: {resp.text}\"}\n",
    "        except Exception as e:\n",
    "            print(f\"Cario API Exception for {job_id}: {e}\")\n",
    "            results[str(job_id)] = {\"error\": str(e)}\n",
    "    return results\n",
    "\n",
    "def _get_bigpost_tracking_raw_final(job_ids: List[str], creds: Dict) -> Dict:  # FIXED: Return type\n",
    "    url = f\"{creds['base_url'].rstrip('/')}/api/gettracking\"\n",
    "    headers = {\"AccessToken\": creds['access_token'], \"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=job_ids, timeout=25)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()  # Returns dict with \"Object\" and \"Errors\" keys\n",
    "    except Exception as e:\n",
    "        print(f\"BigPost API Error: {e}\")\n",
    "        return {\"Object\": [], \"Errors\": [{\"error\": str(e)}]}\n",
    "\n",
    "\n",
    "# --- 🔄 Layer 2: Transformation (CORRECTED) ---\n",
    "def _safe_parse_iso_final(ts: Optional[str]) -> Optional[datetime]:\n",
    "    if not ts: return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def _map_to_canonical_status_final(platform: str, raw_status: str) -> str:\n",
    "    if not isinstance(raw_status, str): return \"UNKNOWN\"\n",
    "    s = raw_status.lower()\n",
    "    if \"delivered\" in s or \"complete\" in s or \"pod received\" in s: return \"DELIVERED\"\n",
    "    if \"delivery\" in s or \"on board for delivery\" in s: return \"OUT_FOR_DELIVERY\"\n",
    "    if \"picked up\" in s or \"in transit\" in s or \"at depot\" in s or \"scanned\" in s: return \"IN_TRANSIT\"\n",
    "    if \"booked\" in s or \"manifested\" in s or \"unmanifested\" in s: return \"BOOKED\"\n",
    "    if \"pending\" in s or \"created\" in s: return \"PENDING\"\n",
    "    if \"exception\" in s or \"failed\" in s or \"issue\" in s: return \"EXCEPTION\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def _transform_machship_data_final(raw_data: Dict) -> Dict:\n",
    "    harmonized = {}\n",
    "    for history in raw_data.get(\"object\", []):\n",
    "        job_id = str(history.get(\"consignmentId\"))\n",
    "        events = []\n",
    "        for s in history.get(\"statusHistory\", []):\n",
    "            machship_status = s.get(\"consignmentTrackingStatus\", {}).get(\"name\")\n",
    "            carrier_status = s.get(\"carrierStatusDescription\")\n",
    "            event = {\n",
    "                \"timestamp_utc\": _safe_parse_iso_final(s.get(\"statusDateUtc\")),\n",
    "                \"standard_status\": _map_to_canonical_status_final(\"MachShip\", machship_status or carrier_status),\n",
    "                \"details\": f\"Machship: {machship_status} / Carrier: {carrier_status}\",\n",
    "            }\n",
    "            events.append(event)\n",
    "        harmonized[job_id] = sorted(events, key=lambda x: x[\"timestamp_utc\"] or datetime.min)\n",
    "    return harmonized\n",
    "\n",
    "def _transform_cario_data_final(raw_data: Dict) -> Dict:\n",
    "    harmonized = {}\n",
    "    for job_id, data_list in raw_data.items():\n",
    "        # Skip if there was an error\n",
    "        if isinstance(data_list, dict) and \"error\" in data_list:\n",
    "            print(f\"Skipping Cario job_id {job_id} due to error\")\n",
    "            continue\n",
    "            \n",
    "        # Cario can return a list of consignments, we'll take the first.\n",
    "        data = data_list[0] if isinstance(data_list, list) and data_list else data_list\n",
    "        if not data or \"error\" in data: \n",
    "            continue\n",
    "        \n",
    "        events = []\n",
    "        for s in data.get(\"events\", []):\n",
    "            status_text = s.get(\"milestone\") or s.get(\"comments\")\n",
    "            date_text = s.get(\"eventTime\")\n",
    "            event = {\n",
    "                \"timestamp_utc\": _safe_parse_iso_final(date_text),\n",
    "                \"standard_status\": _map_to_canonical_status_final(\"Cario\", status_text),\n",
    "                \"details\": status_text,\n",
    "            }\n",
    "            events.append(event)\n",
    "        harmonized[job_id] = sorted(events, key=lambda x: x[\"timestamp_utc\"] or datetime.min)\n",
    "    return harmonized\n",
    "\n",
    "def _transform_bigpost_data_final(raw_data: Dict) -> Dict:  # FIXED: Expect dict, not list\n",
    "    harmonized = {}\n",
    "    \n",
    "    # FIXED: Handle the actual BigPost API response structure\n",
    "    if not isinstance(raw_data, dict):\n",
    "        print(f\"Warning: Expected dict, got {type(raw_data)}\")\n",
    "        return {}\n",
    "    \n",
    "    # BigPost returns {\"Object\": [...], \"Errors\": [...]}\n",
    "    items = raw_data.get(\"Object\", [])\n",
    "    if not isinstance(items, list):\n",
    "        print(f\"Warning: Expected Object to be list, got {type(items)}\")\n",
    "        return {}\n",
    "    \n",
    "    for item in items:\n",
    "        if \"error\" in item: \n",
    "            continue\n",
    "            \n",
    "        # FIXED: Use correct key name from actual API response\n",
    "        job_id = str(item.get(\"JobId\"))  # BigPost uses \"JobId\", confirmed from debug\n",
    "        \n",
    "        events = []\n",
    "        for s in item.get(\"StatusHistory\", []):\n",
    "            status_text = s.get(\"Status\")\n",
    "            date_text = s.get(\"Date\")\n",
    "            event = {\n",
    "                \"timestamp_utc\": _safe_parse_iso_final(date_text),\n",
    "                \"standard_status\": _map_to_canonical_status_final(\"BigPost\", status_text),\n",
    "                \"details\": status_text,\n",
    "            }\n",
    "            events.append(event)\n",
    "            \n",
    "        harmonized[job_id] = sorted(events, key=lambda x: x[\"timestamp_utc\"] or datetime.min)\n",
    "        \n",
    "    return harmonized\n",
    "\n",
    "\n",
    "# --- 🚚 Layer 3: Orchestrator (CORRECTED) ---\n",
    "def get_unified_tracking_final(jobs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not all(col in jobs_df.columns for col in ['job_id', 'platform']):\n",
    "        raise ValueError(\"Input DataFrame must contain 'job_id' and 'platform' columns.\")\n",
    "        \n",
    "    all_harmonized_data = {}\n",
    "\n",
    "    for platform, group in jobs_df.groupby('platform'):\n",
    "        print(f\"Fetching data for {len(group)} jobs from {platform}...\")\n",
    "        job_ids = group['job_id'].astype(str).tolist()\n",
    "        \n",
    "        if platform == 'MachShip':\n",
    "            raw_data = _get_machship_tracking_raw_final(job_ids, CREDENTIALS_FINAL['MachShip'])\n",
    "            harmonized_batch = _transform_machship_data_final(raw_data)\n",
    "        elif platform == 'Cario':\n",
    "            raw_data = _get_cario_tracking_raw_final(job_ids, CREDENTIALS_FINAL['Cario'])\n",
    "            harmonized_batch = _transform_cario_data_final(raw_data)\n",
    "        elif platform == 'BigPost':\n",
    "            raw_data = _get_bigpost_tracking_raw_final(job_ids, CREDENTIALS_FINAL['BigPost'])\n",
    "            harmonized_batch = _transform_bigpost_data_final(raw_data)  # Now correctly processes dict\n",
    "        else:\n",
    "            print(f\"Warning: Unknown platform '{platform}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        all_harmonized_data.update(harmonized_batch)\n",
    "\n",
    "    # --- LOAD (Enrich DataFrame) ---\n",
    "    def get_latest_status(job_id):\n",
    "        history = all_harmonized_data.get(str(job_id))\n",
    "        return history[-1] if history else {}\n",
    "\n",
    "    latest_statuses = jobs_df['job_id'].apply(get_latest_status)\n",
    "    \n",
    "    df = jobs_df.copy()\n",
    "    df['latest_status'] = [s.get('standard_status', 'NO_DATA') for s in latest_statuses]\n",
    "    \n",
    "    # FIXED: Convert timezone-aware datetimes to UTC before passing to pandas\n",
    "    timestamps = []\n",
    "    for s in latest_statuses:\n",
    "        timestamp = s.get('timestamp_utc')\n",
    "        if timestamp and hasattr(timestamp, 'replace'):\n",
    "            # Convert timezone-aware datetime to UTC naive datetime\n",
    "            timestamps.append(timestamp.replace(tzinfo=None))\n",
    "        else:\n",
    "            timestamps.append(timestamp)\n",
    "    \n",
    "    df['latest_update_utc'] = pd.to_datetime(timestamps, utc=True)\n",
    "    df['latest_details'] = [s.get('details') for s in latest_statuses]\n",
    "    df['full_tracking_history'] = df['job_id'].apply(lambda x: all_harmonized_data.get(str(x)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- 🚀 Test All Platforms ---\n",
    "jobs_to_track = pd.DataFrame({\n",
    "    'job_id': [\"52768539\", \"8879850005089\", \"2293867\"],\n",
    "    'platform': [\"MachShip\", \"Cario\", \"BigPost\"],\n",
    "    'customer_order_ref': [\"ORD-1001\", \"ORD-1002\", \"ORD-1003\"]\n",
    "})\n",
    "\n",
    "print(\"=== Testing FINAL CORRECTED Unified Tracking ===\")\n",
    "enriched_df = get_unified_tracking_final(jobs_to_track)\n",
    "\n",
    "print(\"\\n--- ✅ FINAL CORRECTED Enriched Tracking DataFrame ---\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 14:25:57,912 - INFO - === TRACKING SESSION STARTED: 20250829_142557_d0e74e67 ===\n",
      "/var/folders/3c/jvdgg4gd113blslzb1sm88mc0000gn/T/ipykernel_81895/2724709399.py:506: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  jobs_with_dates['date'] = pd.to_datetime(jobs_with_dates['date'])\n",
      "2025-08-29 14:25:57,952 - INFO - Starting unified tracking for 3 jobs across 3 platforms\n",
      "2025-08-29 14:25:57,954 - INFO - Processing 1 jobs from BigPost...\n",
      "2025-08-29 14:25:57,958 - INFO - BigPost API - Requesting 1 jobs: ['2293867']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Unified Tracking with Comprehensive Logging ===\n",
      "Session ID: 20250829_142557_d0e74e67\n",
      "Logs will be saved to: tracking_logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 14:25:58,203 - INFO - BigPost API - Job 2293867: Status 200, Data logged\n",
      "2025-08-29 14:25:58,203 - INFO - BigPost - Extracted 5 unique status strings\n",
      "2025-08-29 14:25:58,204 - INFO - Completed processing BigPost - 1 jobs processed\n",
      "2025-08-29 14:25:58,205 - INFO - Processing 1 jobs from Cario...\n",
      "2025-08-29 14:25:58,205 - INFO - Cario API - Requesting job 8879850005089\n",
      "2025-08-29 14:25:59,793 - INFO - Cario API - Job 8879850005089: Status 200, Data logged\n",
      "2025-08-29 14:25:59,793 - INFO - Cario Job 8879850005089 - Extracted 12 unique status strings: ['Delivered', 'FREIGHT DELIVERED BIG-62620', 'On Board for Delivery', 'FREIGHT PICKUP BIG-62620', 'ON FOR DELIVERY BIG-62620', 'Into Depot BIG-62620', 'Shipment notification sent to k....@gmail.com', 'Ready for pickup', 'Collected from Sender', 'ETRADER LOADED (PRIORITY) BIG-62620', 'Shipment Created BIG-62620', 'In Transit']\n",
      "2025-08-29 14:25:59,794 - INFO - Completed processing Cario - 1 jobs processed\n",
      "2025-08-29 14:25:59,795 - INFO - Processing 1 jobs from MachShip...\n",
      "2025-08-29 14:25:59,796 - INFO - MachShip API - Requesting 1 jobs: ['52768539']\n",
      "2025-08-29 14:25:59,932 - INFO - MachShip API - Job 52768539: Status 200, Data logged\n",
      "2025-08-29 14:25:59,932 - INFO - MachShip - Extracted 11 unique status strings\n",
      "2025-08-29 14:25:59,933 - INFO - Completed processing MachShip - 1 jobs processed\n",
      "2025-08-29 14:25:59,935 - INFO - Unified tracking completed - 3 total jobs processed\n",
      "2025-08-29 14:25:59,936 - INFO - Generating status mapping report...\n",
      "2025-08-29 14:25:59,938 - INFO - Status mapping report saved to: tracking_logs/status_mapping_report_20250829_142557_d0e74e67.json\n",
      "2025-08-29 14:25:59,938 - INFO - Report summary: {'total_unique_raw_statuses': 20, 'total_unique_mapped_statuses': 6, 'total_mappings': 20}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ Enhanced Tracking Results ---\n",
      "📁 All logs saved to: tracking_logs\n",
      "📊 Status mapping report: status_mapping_report_20250829_142557_d0e74e67.json\n",
      "📋 20 unique raw statuses found\n",
      "🎯 6 canonical statuses mapped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>customer_order_ref</th>\n",
       "      <th>latest_status</th>\n",
       "      <th>latest_update_utc</th>\n",
       "      <th>latest_details</th>\n",
       "      <th>latest_raw_status</th>\n",
       "      <th>full_tracking_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52768539</td>\n",
       "      <td>MachShip</td>\n",
       "      <td>ORD-1001</td>\n",
       "      <td>DELIVERED</td>\n",
       "      <td>2025-08-28 05:46:00+00:00</td>\n",
       "      <td>Machship: Complete / Carrier: Delivered</td>\n",
       "      <td>Complete / Delivered</td>\n",
       "      <td>[{'timestamp_utc': None, 'standard_status': 'BOOKED', 'details': 'Machship: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8879850005089</td>\n",
       "      <td>Cario</td>\n",
       "      <td>ORD-1002</td>\n",
       "      <td>DELIVERED</td>\n",
       "      <td>2025-08-14 11:41:00+00:00</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>[{'timestamp_utc': 2025-08-11 15:47:00+00:00, 'standard_status': 'UNKNOWN', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293867</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>ORD-1003</td>\n",
       "      <td>DELIVERED</td>\n",
       "      <td>2025-08-15 12:04:42+00:00</td>\n",
       "      <td>Business Delivered</td>\n",
       "      <td>Business Delivered</td>\n",
       "      <td>[{'timestamp_utc': None, 'standard_status': 'DELIVERED', 'details': 'Incompl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          job_id  platform customer_order_ref latest_status  \\\n",
       "0       52768539  MachShip           ORD-1001     DELIVERED   \n",
       "1  8879850005089     Cario           ORD-1002     DELIVERED   \n",
       "2        2293867   BigPost           ORD-1003     DELIVERED   \n",
       "\n",
       "          latest_update_utc                           latest_details  \\\n",
       "0 2025-08-28 05:46:00+00:00  Machship: Complete / Carrier: Delivered   \n",
       "1 2025-08-14 11:41:00+00:00                                Delivered   \n",
       "2 2025-08-15 12:04:42+00:00                       Business Delivered   \n",
       "\n",
       "      latest_raw_status  \\\n",
       "0  Complete / Delivered   \n",
       "1             Delivered   \n",
       "2    Business Delivered   \n",
       "\n",
       "                                                             full_tracking_history  \n",
       "0  [{'timestamp_utc': None, 'standard_status': 'BOOKED', 'details': 'Machship: ...  \n",
       "1  [{'timestamp_utc': 2025-08-11 15:47:00+00:00, 'standard_status': 'UNKNOWN', ...  \n",
       "2  [{'timestamp_utc': None, 'standard_status': 'DELIVERED', 'details': 'Incompl...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  ENHANCED Unified Freight Tracking Hub with Comprehensive Response Logging\n",
    "# ==============================================================================\n",
    "#\n",
    "#  🆕 NEW FEATURES:\n",
    "#  - Complete API response logging to JSON files\n",
    "#  - Raw status extraction for canonical mapping analysis\n",
    "#  - Timestamped logging with session IDs\n",
    "#  - Response statistics and summary reports\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "# --- 📊 Enhanced Logging Setup ---\n",
    "def setup_comprehensive_logging():\n",
    "    \"\"\"Set up comprehensive logging for API responses and analysis\"\"\"\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_dir = Path(\"tracking_logs\")\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create session ID for this run\n",
    "    session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + str(uuid.uuid4())[:8]\n",
    "    \n",
    "    # Set up file logging\n",
    "    log_file = log_dir / f\"tracking_session_{session_id}.log\"\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True  # Override any existing configuration\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"=== TRACKING SESSION STARTED: {session_id} ===\")\n",
    "    \n",
    "    return logger, session_id, log_dir\n",
    "\n",
    "# Initialize logging\n",
    "logger, SESSION_ID, LOG_DIR = setup_comprehensive_logging()\n",
    "\n",
    "# --- ⚙️ Credentials & Configuration ---\n",
    "CREDENTIALS_ENHANCED = {\n",
    "    \"MachShip\": {\n",
    "        \"api_token\": \"MkfE0KS0GU6_RG4UC5P1bQfnmF_M9HB02uj_PpuwtpCQ\",\n",
    "        \"base_url\": \"https://live.machship.com\"\n",
    "    },\n",
    "    \"Cario\": {\n",
    "        \"auth_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1laWRlbnRpZmllciI6Ijk5MDkiLCJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1lIjoiWl9HcmlsbHNfQXVzdHJhbGlhIiwiaHR0cDovL3NjaGVtYXMueG1sc29hcC5vcmcvd3MvMjAwNS8wNS9pZGVudGl0eS9jbGFpbXMvZW1haWxhZGRyZXNzIjoianVzdGluLm5nQHpncmlsbHMuY29tLmF1IiwiQXNwTmV0LklkZW50aXR5LlNlY3VyaXR5U3RhbXAiOiJNNVBVUFVZSUhYSVBYQ0ZSNVBDVjJWWVNMRFVUNFQ2QSIsImh0dHA6Ly9zY2hlbWFzLm1pY3Jvc29mdC5jb20vd3MvMjAwOC8wNi9pZGVudGl0eS9jbGFpbXMvcm9sZSI6IkRlc3BhdGNoUGFyZW50QWNjdCIsImh0dHA6Ly93d3cuYXNwbmV0Ym9pbGVycGxhdGUuY29tL2lkZW50aXR5L2NsYWltcy90ZW5hbnRJZCI6IjkiLCJDdXN0b21lcklEIjoiNzExNSIsInN1YiI6Ijk5MDkiLCJqdGkiOiIyZTlhMWIzMi1kZDg1LTRkM2UtOTZiYS1iMmY5ZWExMTk1YWMiLCJpYXQiOjE3NTYzNjExMTIsIm5iZiI6MTc1NjM2MTExMiwiZXhwIjoxNzU2NDQ3NTEyLCJpc3MiOiJGTVMiLCJhdWQiOiJGTVMifQ.gpX61fJ9CCpYX_BQFj5SDhJam9hV_1w-dRvCyayOJuk\",\n",
    "        \"customer_id\": \"7115\",\n",
    "        \"tenant_id\": \"\",\n",
    "        \"base_url\": \"https://integrate.cario.com.au/api/\"\n",
    "    },\n",
    "    \"BigPost\": {\n",
    "        \"access_token\": \"26lyJ90vNka0wPOJtmPcbw\",\n",
    "        \"base_url\": \"https://api.bigpost.com.au\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 📋 Response Logging Functions ---\n",
    "def log_api_response(platform: str, job_id: str, response_data: Any, status_code: int = None, error: str = None):\n",
    "    \"\"\"Log complete API response for analysis\"\"\"\n",
    "    \n",
    "    log_entry = {\n",
    "        \"session_id\": SESSION_ID,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"platform\": platform,\n",
    "        \"job_id\": job_id,\n",
    "        \"status_code\": status_code,\n",
    "        \"error\": error,\n",
    "        \"response_data\": response_data\n",
    "    }\n",
    "    \n",
    "    # Save to platform-specific file\n",
    "    response_file = LOG_DIR / f\"responses_{platform.lower()}_{SESSION_ID}.jsonl\"\n",
    "    with open(response_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(log_entry, default=str, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    logger.info(f\"{platform} API - Job {job_id}: Status {status_code}, Data logged\")\n",
    "\n",
    "def extract_raw_statuses(platform: str, response_data: Any) -> List[str]:\n",
    "    \"\"\"Extract all unique raw status strings from API response for mapping analysis\"\"\"\n",
    "    \n",
    "    statuses = set()\n",
    "    \n",
    "    try:\n",
    "        if platform == \"MachShip\":\n",
    "            for history in response_data.get(\"object\", []):\n",
    "                for status_entry in history.get(\"statusHistory\", []):\n",
    "                    machship_status = status_entry.get(\"consignmentTrackingStatus\", {}).get(\"name\")\n",
    "                    carrier_status = status_entry.get(\"carrierStatusDescription\")\n",
    "                    if machship_status:\n",
    "                        statuses.add(f\"MachShip: {machship_status}\")\n",
    "                    if carrier_status:\n",
    "                        statuses.add(f\"Carrier: {carrier_status}\")\n",
    "        \n",
    "        elif platform == \"Cario\":\n",
    "            if isinstance(response_data, list):\n",
    "                data_list = response_data\n",
    "            else:\n",
    "                data_list = [response_data] if response_data else []\n",
    "            \n",
    "            for data in data_list:\n",
    "                for event in data.get(\"events\", []):\n",
    "                    milestone = event.get(\"milestone\")\n",
    "                    comments = event.get(\"comments\")\n",
    "                    if milestone:\n",
    "                        statuses.add(milestone)\n",
    "                    if comments:\n",
    "                        statuses.add(comments)\n",
    "        \n",
    "        elif platform == \"BigPost\":\n",
    "            for item in response_data.get(\"Object\", []):\n",
    "                for status_entry in item.get(\"StatusHistory\", []):\n",
    "                    status = status_entry.get(\"Status\")\n",
    "                    if status:\n",
    "                        statuses.add(status)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting statuses from {platform}: {e}\")\n",
    "    \n",
    "    return list(statuses)\n",
    "\n",
    "def log_status_analysis(platform: str, job_id: str, raw_statuses: List[str], mapped_statuses: List[str]):\n",
    "    \"\"\"Log status mapping analysis\"\"\"\n",
    "    \n",
    "    analysis_entry = {\n",
    "        \"session_id\": SESSION_ID,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"platform\": platform,\n",
    "        \"job_id\": job_id,\n",
    "        \"raw_statuses\": raw_statuses,\n",
    "        \"mapped_statuses\": mapped_statuses,\n",
    "        \"unique_raw_count\": len(set(raw_statuses)),\n",
    "        \"unique_mapped_count\": len(set(mapped_statuses))\n",
    "    }\n",
    "    \n",
    "    analysis_file = LOG_DIR / f\"status_analysis_{SESSION_ID}.jsonl\"\n",
    "    with open(analysis_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(analysis_entry, default=str, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# --- 🌐 Enhanced API Connectors with Comprehensive Logging ---\n",
    "def _get_machship_tracking_raw_enhanced(job_ids: List[str], creds: Dict) -> Dict:\n",
    "    url = f\"{creds['base_url'].rstrip('/')}/apiv2/consignments/returnConsignmentStatuses\"\n",
    "    headers = {\"token\": creds['api_token'], \"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    logger.info(f\"MachShip API - Requesting {len(job_ids)} jobs: {job_ids}\")\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json={\"ids\": [int(j) for j in job_ids]}, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        response_data = resp.json()\n",
    "        \n",
    "        # Log the complete response\n",
    "        log_api_response(\"MachShip\", \",\".join(job_ids), response_data, resp.status_code)\n",
    "        \n",
    "        # Extract and log raw statuses\n",
    "        raw_statuses = extract_raw_statuses(\"MachShip\", response_data)\n",
    "        logger.info(f\"MachShip - Extracted {len(raw_statuses)} unique status strings\")\n",
    "        \n",
    "        return response_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"MachShip API Error: {e}\")\n",
    "        log_api_response(\"MachShip\", \",\".join(job_ids), None, None, str(e))\n",
    "        return {\"errors\": [{\"errorMessage\": str(e)}]}\n",
    "\n",
    "def _get_cario_tracking_raw_enhanced(job_ids: List[str], creds: Dict) -> Dict:\n",
    "    results = {}\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {creds['auth_token']}\", \n",
    "        \"CustomerId\": creds['customer_id'], \n",
    "        \"TenantId\": creds.get('tenant_id', ''),\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    for job_id in job_ids:\n",
    "        url = f\"{creds['base_url']}/ConsignmentStatus/Query/{job_id}\"\n",
    "        logger.info(f\"Cario API - Requesting job {job_id}\")\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=30)\n",
    "            \n",
    "            if resp.ok:\n",
    "                response_data = resp.json()\n",
    "                results[str(job_id)] = response_data\n",
    "                \n",
    "                # Log the complete response\n",
    "                log_api_response(\"Cario\", job_id, response_data, resp.status_code)\n",
    "                \n",
    "                # Extract and log raw statuses\n",
    "                raw_statuses = extract_raw_statuses(\"Cario\", response_data)\n",
    "                logger.info(f\"Cario Job {job_id} - Extracted {len(raw_statuses)} unique status strings: {raw_statuses}\")\n",
    "                \n",
    "            else:\n",
    "                error_msg = f\"HTTP {resp.status_code}: {resp.text}\"\n",
    "                results[str(job_id)] = {\"error\": error_msg}\n",
    "                log_api_response(\"Cario\", job_id, None, resp.status_code, error_msg)\n",
    "                logger.warning(f\"Cario Job {job_id} - {error_msg}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            results[str(job_id)] = {\"error\": error_msg}\n",
    "            log_api_response(\"Cario\", job_id, None, None, error_msg)\n",
    "            logger.error(f\"Cario Job {job_id} - Exception: {error_msg}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _get_bigpost_tracking_raw_enhanced(job_ids: List[str], creds: Dict) -> Dict:\n",
    "    url = f\"{creds['base_url'].rstrip('/')}/api/gettracking\"\n",
    "    headers = {\"AccessToken\": creds['access_token'], \"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
    "    \n",
    "    logger.info(f\"BigPost API - Requesting {len(job_ids)} jobs: {job_ids}\")\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=job_ids, timeout=25)\n",
    "        resp.raise_for_status()\n",
    "        response_data = resp.json()\n",
    "        \n",
    "        # Log the complete response\n",
    "        log_api_response(\"BigPost\", \",\".join(job_ids), response_data, resp.status_code)\n",
    "        \n",
    "        # Extract and log raw statuses\n",
    "        raw_statuses = extract_raw_statuses(\"BigPost\", response_data)\n",
    "        logger.info(f\"BigPost - Extracted {len(raw_statuses)} unique status strings\")\n",
    "        \n",
    "        return response_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"BigPost API Error: {e}\")\n",
    "        log_api_response(\"BigPost\", \",\".join(job_ids), None, None, str(e))\n",
    "        return {\"Object\": [], \"Errors\": [{\"error\": str(e)}]}\n",
    "\n",
    "# --- 🔄 Enhanced Transformation with Status Analysis ---\n",
    "def _safe_parse_iso_enhanced(ts: Optional[str]) -> Optional[datetime]:\n",
    "    if not ts: return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def _map_to_canonical_status_enhanced(platform: str, raw_status: str) -> str:\n",
    "    \"\"\"Enhanced status mapping with logging\"\"\"\n",
    "    if not isinstance(raw_status, str): \n",
    "        return \"UNKNOWN\"\n",
    "    \n",
    "    s = raw_status.lower()\n",
    "    \n",
    "    # Current mapping logic (you can improve this based on logged data)\n",
    "    if \"delivered\" in s or \"complete\" in s or \"pod received\" in s: \n",
    "        mapped = \"DELIVERED\"\n",
    "    elif \"delivery\" in s or \"on board for delivery\" in s: \n",
    "        mapped = \"OUT_FOR_DELIVERY\"\n",
    "    elif \"picked up\" in s or \"in transit\" in s or \"at depot\" in s or \"scanned\" in s: \n",
    "        mapped = \"IN_TRANSIT\"\n",
    "    elif \"booked\" in s or \"manifested\" in s or \"unmanifested\" in s: \n",
    "        mapped = \"BOOKED\"\n",
    "    elif \"pending\" in s or \"created\" in s: \n",
    "        mapped = \"PENDING\"\n",
    "    elif \"exception\" in s or \"failed\" in s or \"issue\" in s: \n",
    "        mapped = \"EXCEPTION\"\n",
    "    else: \n",
    "        mapped = \"UNKNOWN\"\n",
    "    \n",
    "    # Log the mapping for analysis\n",
    "    logger.debug(f\"Status Mapping - Platform: {platform}, Raw: '{raw_status}' -> Canonical: '{mapped}'\")\n",
    "    \n",
    "    return mapped\n",
    "\n",
    "def _transform_machship_data_enhanced(raw_data: Dict) -> Dict:\n",
    "    harmonized = {}\n",
    "    for history in raw_data.get(\"object\", []):\n",
    "        job_id = str(history.get(\"consignmentId\"))\n",
    "        events = []\n",
    "        raw_statuses = []\n",
    "        mapped_statuses = []\n",
    "        \n",
    "        for s in history.get(\"statusHistory\", []):\n",
    "            machship_status = s.get(\"consignmentTrackingStatus\", {}).get(\"name\")\n",
    "            carrier_status = s.get(\"carrierStatusDescription\")\n",
    "            \n",
    "            combined_status = f\"{machship_status or ''} / {carrier_status or ''}\".strip(\" /\")\n",
    "            raw_statuses.append(combined_status)\n",
    "            \n",
    "            mapped_status = _map_to_canonical_status_enhanced(\"MachShip\", machship_status or carrier_status or \"\")\n",
    "            mapped_statuses.append(mapped_status)\n",
    "            \n",
    "            event = {\n",
    "                \"timestamp_utc\": _safe_parse_iso_enhanced(s.get(\"statusDateUtc\")),\n",
    "                \"standard_status\": mapped_status,\n",
    "                \"details\": f\"Machship: {machship_status} / Carrier: {carrier_status}\",\n",
    "                \"raw_status\": combined_status\n",
    "            }\n",
    "            events.append(event)\n",
    "        \n",
    "        # Log status analysis\n",
    "        log_status_analysis(\"MachShip\", job_id, raw_statuses, mapped_statuses)\n",
    "        \n",
    "        harmonized[job_id] = sorted(events, key=lambda x: x[\"timestamp_utc\"] or datetime.min)\n",
    "    \n",
    "    return harmonized\n",
    "\n",
    "def _transform_cario_data_enhanced(raw_data: Dict) -> Dict:\n",
    "    harmonized = {}\n",
    "    for job_id, data_list in raw_data.items():\n",
    "        if isinstance(data_list, dict) and \"error\" in data_list:\n",
    "            logger.warning(f\"Skipping Cario job_id {job_id} due to error: {data_list.get('error')}\")\n",
    "            continue\n",
    "            \n",
    "        data = data_list[0] if isinstance(data_list, list) and data_list else data_list\n",
    "        if not data or \"error\" in data: \n",
    "            continue\n",
    "        \n",
    "        events = []\n",
    "        raw_statuses = []\n",
    "        mapped_statuses = []\n",
    "        \n",
    "        for s in data.get(\"events\", []):\n",
    "            status_text = s.get(\"milestone\") or s.get(\"comments\") or \"\"\n",
    "            raw_statuses.append(status_text)\n",
    "            \n",
    "            mapped_status = _map_to_canonical_status_enhanced(\"Cario\", status_text)\n",
    "            mapped_statuses.append(mapped_status)\n",
    "            \n",
    "            event = {\n",
    "                \"timestamp_utc\": _safe_parse_iso_enhanced(s.get(\"eventTime\")),\n",
    "                \"standard_status\": mapped_status,\n",
    "                \"details\": status_text,\n",
    "                \"raw_status\": status_text\n",
    "            }\n",
    "            events.append(event)\n",
    "        \n",
    "        # Log status analysis\n",
    "        log_status_analysis(\"Cario\", job_id, raw_statuses, mapped_statuses)\n",
    "        \n",
    "        harmonized[job_id] = sorted(events, key=lambda x: x[\"timestamp_utc\"] or datetime.min)\n",
    "    \n",
    "    return harmonized\n",
    "\n",
    "def _transform_bigpost_data_enhanced(raw_data: Dict) -> Dict:\n",
    "    harmonized = {}\n",
    "    \n",
    "    if not isinstance(raw_data, dict):\n",
    "        logger.warning(f\"BigPost: Expected dict, got {type(raw_data)}\")\n",
    "        return {}\n",
    "    \n",
    "    items = raw_data.get(\"Object\", [])\n",
    "    if not isinstance(items, list):\n",
    "        logger.warning(f\"BigPost: Expected Object to be list, got {type(items)}\")\n",
    "        return {}\n",
    "    \n",
    "    for item in items:\n",
    "        if \"error\" in item: \n",
    "            continue\n",
    "            \n",
    "        job_id = str(item.get(\"JobId\"))\n",
    "        events = []\n",
    "        raw_statuses = []\n",
    "        mapped_statuses = []\n",
    "        \n",
    "        for s in item.get(\"StatusHistory\", []):\n",
    "            status_text = s.get(\"Status\") or \"\"\n",
    "            raw_statuses.append(status_text)\n",
    "            \n",
    "            mapped_status = _map_to_canonical_status_enhanced(\"BigPost\", status_text)\n",
    "            mapped_statuses.append(mapped_status)\n",
    "            \n",
    "            event = {\n",
    "                \"timestamp_utc\": _safe_parse_iso_enhanced(s.get(\"Date\")),\n",
    "                \"standard_status\": mapped_status,\n",
    "                \"details\": status_text,\n",
    "                \"raw_status\": status_text\n",
    "            }\n",
    "            events.append(event)\n",
    "        \n",
    "        # Log status analysis\n",
    "        log_status_analysis(\"BigPost\", job_id, raw_statuses, mapped_statuses)\n",
    "        \n",
    "        harmonized[job_id] = sorted(events, key=lambda x: x[\"timestamp_utc\"] or datetime.min)\n",
    "        \n",
    "    return harmonized\n",
    "\n",
    "# --- 🚚 Enhanced Orchestrator ---\n",
    "def get_unified_tracking_enhanced(jobs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not all(col in jobs_df.columns for col in ['job_id', 'platform']):\n",
    "        raise ValueError(\"Input DataFrame must contain 'job_id' and 'platform' columns.\")\n",
    "    \n",
    "    logger.info(f\"Starting unified tracking for {len(jobs_df)} jobs across {jobs_df['platform'].nunique()} platforms\")\n",
    "    \n",
    "    all_harmonized_data = {}\n",
    "\n",
    "    for platform, group in jobs_df.groupby('platform'):\n",
    "        logger.info(f\"Processing {len(group)} jobs from {platform}...\")\n",
    "        job_ids = group['job_id'].astype(str).tolist()\n",
    "        \n",
    "        if platform == 'MachShip':\n",
    "            raw_data = _get_machship_tracking_raw_enhanced(job_ids, CREDENTIALS_ENHANCED['MachShip'])\n",
    "            harmonized_batch = _transform_machship_data_enhanced(raw_data)\n",
    "        elif platform == 'Cario':\n",
    "            raw_data = _get_cario_tracking_raw_enhanced(job_ids, CREDENTIALS_ENHANCED['Cario'])\n",
    "            harmonized_batch = _transform_cario_data_enhanced(raw_data)\n",
    "        elif platform == 'BigPost':\n",
    "            raw_data = _get_bigpost_tracking_raw_enhanced(job_ids, CREDENTIALS_ENHANCED['BigPost'])\n",
    "            harmonized_batch = _transform_bigpost_data_enhanced(raw_data)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown platform '{platform}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        all_harmonized_data.update(harmonized_batch)\n",
    "        logger.info(f\"Completed processing {platform} - {len(harmonized_batch)} jobs processed\")\n",
    "\n",
    "    # Build result DataFrame\n",
    "    def get_latest_status(job_id):\n",
    "        history = all_harmonized_data.get(str(job_id))\n",
    "        return history[-1] if history else {}\n",
    "\n",
    "    latest_statuses = jobs_df['job_id'].apply(get_latest_status)\n",
    "    \n",
    "    df = jobs_df.copy()\n",
    "    df['latest_status'] = [s.get('standard_status', 'NO_DATA') for s in latest_statuses]\n",
    "    \n",
    "    timestamps = []\n",
    "    for s in latest_statuses:\n",
    "        timestamp = s.get('timestamp_utc')\n",
    "        if timestamp and hasattr(timestamp, 'replace'):\n",
    "            timestamps.append(timestamp.replace(tzinfo=None))\n",
    "        else:\n",
    "            timestamps.append(timestamp)\n",
    "    \n",
    "    df['latest_update_utc'] = pd.to_datetime(timestamps, utc=True)\n",
    "    df['latest_details'] = [s.get('details') for s in latest_statuses]\n",
    "    df['latest_raw_status'] = [s.get('raw_status') for s in latest_statuses]  # NEW: Raw status field\n",
    "    df['full_tracking_history'] = df['job_id'].apply(lambda x: all_harmonized_data.get(str(x)))\n",
    "\n",
    "    logger.info(f\"Unified tracking completed - {len(df)} total jobs processed\")\n",
    "    return df\n",
    "\n",
    "# --- 📊 Analysis and Reporting Functions ---\n",
    "def generate_status_mapping_report():\n",
    "    \"\"\"Generate a comprehensive report of all status mappings for analysis\"\"\"\n",
    "    \n",
    "    logger.info(\"Generating status mapping report...\")\n",
    "    \n",
    "    # Read all status analysis files\n",
    "    all_statuses = {\"raw\": set(), \"mapped\": set(), \"mappings\": {}}\n",
    "    \n",
    "    for file_path in LOG_DIR.glob(f\"status_analysis_{SESSION_ID}.jsonl\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                platform = entry['platform']\n",
    "                \n",
    "                for raw, mapped in zip(entry['raw_statuses'], entry['mapped_statuses']):\n",
    "                    all_statuses[\"raw\"].add(raw)\n",
    "                    all_statuses[\"mapped\"].add(mapped)\n",
    "                    \n",
    "                    # Track the mapping\n",
    "                    key = f\"{platform}:{raw}\"\n",
    "                    if key not in all_statuses[\"mappings\"]:\n",
    "                        all_statuses[\"mappings\"][key] = set()\n",
    "                    all_statuses[\"mappings\"][key].add(mapped)\n",
    "    \n",
    "    # Generate report\n",
    "    report = {\n",
    "        \"session_id\": SESSION_ID,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"summary\": {\n",
    "            \"total_unique_raw_statuses\": len(all_statuses[\"raw\"]),\n",
    "            \"total_unique_mapped_statuses\": len(all_statuses[\"mapped\"]),\n",
    "            \"total_mappings\": len(all_statuses[\"mappings\"])\n",
    "        },\n",
    "        \"raw_statuses\": sorted(list(all_statuses[\"raw\"])),\n",
    "        \"mapped_statuses\": sorted(list(all_statuses[\"mapped\"])),\n",
    "        \"mappings\": {k: list(v) for k, v in all_statuses[\"mappings\"].items()}\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_file = LOG_DIR / f\"status_mapping_report_{SESSION_ID}.json\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, default=str, ensure_ascii=False)\n",
    "    \n",
    "    logger.info(f\"Status mapping report saved to: {report_file}\")\n",
    "    logger.info(f\"Report summary: {report['summary']}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# --- 🚀 Enhanced Test with Comprehensive Logging ---\n",
    "# Read jobs from CSV file\n",
    "all_jobs = pd.read_csv('all_jobs.csv')\n",
    "\n",
    "# Filter out jobs without dates and sort by date, then take top 30 most recent\n",
    "jobs_with_dates = all_jobs[all_jobs['date'].notna() & (all_jobs['date'] != '')]\n",
    "jobs_with_dates['date'] = pd.to_datetime(jobs_with_dates['date'])\n",
    "jobs_to_track = jobs_with_dates.sort_values('date', ascending=False).head(30)\n",
    "\n",
    "# Keep only the required columns for tracking\n",
    "jobs_to_track = jobs_to_track[['job_id', 'platform', 'customer_order_ref']].reset_index(drop=True)\n",
    "\n",
    "# --- 🚀 Test All Platforms ---\n",
    "jobs_to_track = pd.DataFrame({\n",
    "    'job_id': [\"52768539\", \"8879850005089\", \"2293867\"],\n",
    "    'platform': [\"MachShip\", \"Cario\", \"BigPost\"],\n",
    "    'customer_order_ref': [\"ORD-1001\", \"ORD-1002\", \"ORD-1003\"]\n",
    "})\n",
    "\n",
    "print(\"=== Enhanced Unified Tracking with Comprehensive Logging ===\")\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "print(f\"Logs will be saved to: {LOG_DIR}\")\n",
    "\n",
    "enriched_df = get_unified_tracking_enhanced(jobs_to_track)\n",
    "\n",
    "# Generate analysis report\n",
    "report = generate_status_mapping_report()\n",
    "\n",
    "print(f\"\\n--- ✅ Enhanced Tracking Results ---\")\n",
    "print(f\"📁 All logs saved to: {LOG_DIR}\")\n",
    "print(f\"📊 Status mapping report: status_mapping_report_{SESSION_ID}.json\")\n",
    "print(f\"📋 {report['summary']['total_unique_raw_statuses']} unique raw statuses found\")\n",
    "print(f\"🎯 {report['summary']['total_unique_mapped_statuses']} canonical statuses mapped\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/jvdgg4gd113blslzb1sm88mc0000gn/T/ipykernel_81895/2609401532.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  jobs_with_dates['date'] = pd.to_datetime(jobs_with_dates['date'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>customer_order_ref</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2293223</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-62648 Ryan Martin</td>\n",
       "      <td>2025-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2293867</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>ZGS-2122 James Oloughlin</td>\n",
       "      <td>2025-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2167778</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG59531 Marie Johnson</td>\n",
       "      <td>2025-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2134314</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-59180 Peter Barbeler</td>\n",
       "      <td>2025-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>2106289</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-58888 Lawson Kemp</td>\n",
       "      <td>2025-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>2105841</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>ZGS-1883 Lewis Rolfe</td>\n",
       "      <td>2025-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2073966</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>ZGS-1835 - Matt Johnston</td>\n",
       "      <td>2025-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2293011</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-62632 Jono Clare</td>\n",
       "      <td>2025-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>2292979</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-62639 Damon Higginbotham</td>\n",
       "      <td>2025-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>2292710</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-62595 Nicholas Stephens</td>\n",
       "      <td>2025-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2260060</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-61854 Mark Mitchell</td>\n",
       "      <td>2025-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2226122</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-61091 Michael Altorfer</td>\n",
       "      <td>2025-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2226112</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-61084 Kelvin Andrijich</td>\n",
       "      <td>2025-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2165448</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG59503 David Mcsweeney</td>\n",
       "      <td>2025-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>2104587</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-58877 Declan Ortner</td>\n",
       "      <td>2025-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>2104551</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-58880 Mark White</td>\n",
       "      <td>2025-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2224349</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG61041 Krista Nichols</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2224155</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>ZGS-2044 - Dean Laird</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2157031</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-59406 Ben Lang</td>\n",
       "      <td>2025-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2165509</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-59498 Richard Tangye</td>\n",
       "      <td>2025-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2165473</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-59505 Blair Richter</td>\n",
       "      <td>2025-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2164092</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG59491 Mathew Pitter</td>\n",
       "      <td>2025-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>2131532</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-59138 Rob Goodwin</td>\n",
       "      <td>2025-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>2131469</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG59141 Andrew Yeates</td>\n",
       "      <td>2025-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2131787</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG59135 Peter Hardin</td>\n",
       "      <td>2025-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>2131506</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG59143 Travis Pope</td>\n",
       "      <td>2025-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2131789</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-59154 Daniel Peacock</td>\n",
       "      <td>2025-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>2103463</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-58875 Chris Bouffler</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>2102768</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-58862 Grant Gibson</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>2102797</td>\n",
       "      <td>BigPost</td>\n",
       "      <td>BIG-58851 Tanya Morel</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_id platform            customer_order_ref       date\n",
       "370  2293223  BigPost         BIG-62648 Ryan Martin 2025-12-08\n",
       "369  2293867  BigPost      ZGS-2122 James Oloughlin 2025-12-08\n",
       "489  2167778  BigPost        BIG59531 Marie Johnson 2025-12-04\n",
       "528  2134314  BigPost      BIG-59180 Peter Barbeler 2025-12-03\n",
       "556  2106289  BigPost         BIG-58888 Lawson Kemp 2025-12-02\n",
       "557  2105841  BigPost          ZGS-1883 Lewis Rolfe 2025-12-02\n",
       "622  2073966  BigPost      ZGS-1835 - Matt Johnston 2025-12-01\n",
       "371  2293011  BigPost          BIG-62632 Jono Clare 2025-11-08\n",
       "372  2292979  BigPost  BIG-62639 Damon Higginbotham 2025-11-08\n",
       "374  2292710  BigPost   BIG-62595 Nicholas Stephens 2025-11-08\n",
       "417  2260060  BigPost       BIG-61854 Mark Mitchell 2025-11-07\n",
       "443  2226122  BigPost    BIG-61091 Michael Altorfer 2025-11-06\n",
       "444  2226112  BigPost    BIG-61084 Kelvin Andrijich 2025-11-06\n",
       "492  2165448  BigPost      BIG59503 David Mcsweeney 2025-11-04\n",
       "558  2104587  BigPost       BIG-58877 Declan Ortner 2025-11-02\n",
       "559  2104551  BigPost          BIG-58880 Mark White 2025-11-02\n",
       "445  2224349  BigPost       BIG61041 Krista Nichols 2025-10-06\n",
       "446  2224155  BigPost         ZGS-2044 - Dean Laird 2025-10-06\n",
       "505  2157031  BigPost            BIG-59406 Ben Lang 2025-10-04\n",
       "490  2165509  BigPost      BIG-59498 Richard Tangye 2025-10-04\n",
       "491  2165473  BigPost       BIG-59505 Blair Richter 2025-10-04\n",
       "495  2164092  BigPost        BIG59491 Mathew Pitter 2025-10-04\n",
       "532  2131532  BigPost         BIG-59138 Rob Goodwin 2025-10-03\n",
       "534  2131469  BigPost        BIG59141 Andrew Yeates 2025-10-03\n",
       "530  2131787  BigPost         BIG59135 Peter Hardin 2025-10-03\n",
       "533  2131506  BigPost          BIG59143 Travis Pope 2025-10-03\n",
       "529  2131789  BigPost      BIG-59154 Daniel Peacock 2025-10-03\n",
       "560  2103463  BigPost      BIG-58875 Chris Bouffler 2025-10-02\n",
       "565  2102768  BigPost        BIG-58862 Grant Gibson 2025-10-02\n",
       "564  2102797  BigPost         BIG-58851 Tanya Morel 2025-10-02"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_jobs = pd.read_csv('all_jobs.csv')\n",
    "\n",
    "# Filter out jobs without dates and sort by date, then take top 30 most recent\n",
    "jobs_with_dates = all_jobs[all_jobs['date'].notna() & (all_jobs['date'] != '')]\n",
    "jobs_with_dates['date'] = pd.to_datetime(jobs_with_dates['date'])\n",
    "jobs_to_track = jobs_with_dates.sort_values('date', ascending=False).head(30)\n",
    "jobs_to_track"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
