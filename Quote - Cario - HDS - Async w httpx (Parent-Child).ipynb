{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinng/opt/miniconda3/envs/shipping/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Declaring all modules and libraries to be used in the project\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys, os, re\n",
    "import random\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from tqdm.notebook import tqdm\n",
    "import function as fn\n",
    "from datetime import datetime\n",
    "from workalendar.oceania.australia import Australia\n",
    "import ipywidgets\n",
    "from fuzzywuzzy import fuzz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "prefix = \"price_to_hds_hand_unload_cario_\"\n",
    "DIMENSIONS = [90, 60, 120, 125, 1] #[L, W, H, Weight, Q]\n",
    "exclusion = [\"\"]\n",
    "\n",
    "request_rates = True\n",
    "id_request = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ▶️  Clean, cache-driven suburb → LocationID loader  (CSV edition)\n",
    "import ast, json, re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz        # pip install rapidfuzz\n",
    "\n",
    "# ------------------------------------------------------------------ helpers\n",
    "STATE_RE = re.compile(r'\\b(?:NSW|QLD|VIC|WA|TAS|SA|ACT|NT)\\b', flags=re.I)\n",
    "\n",
    "def _clean_suburb(txt: str) -> str:\n",
    "    \"\"\"Uniform suburb string: remove state codes/digits, squeeze spaces, upper-case.\"\"\"\n",
    "    txt = STATE_RE.sub('', str(txt))\n",
    "    txt = re.sub(r'\\d+', '', txt)\n",
    "    txt = re.sub(r'\\s+', ' ', txt).strip()\n",
    "    return txt.upper()\n",
    "\n",
    "def _build_loc_df(location_data: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for pc, locs in location_data.items():\n",
    "        pc4 = str(pc).zfill(4)\n",
    "        for loc in locs or []:\n",
    "            sub = loc.get('suburb') or loc.get('description', '')\n",
    "            rows.append(\n",
    "                {\n",
    "                    'postcode': pc4,\n",
    "                    'suburb_clean': _clean_suburb(sub),\n",
    "                    'location_id': loc['id'],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "def assign_ids(df, pc_col, sub_col, out_col, choices_df, fuzz_thresh=90):\n",
    "    \"\"\"Attach Cario location IDs with exact merge, then fuzzy fallback.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['pc4']       = df[pc_col].astype(str).str.zfill(4)\n",
    "    df['sub_clean'] = df[sub_col].map(_clean_suburb)\n",
    "\n",
    "    # exact join -------------------------------------------------------------\n",
    "    df = df.merge(\n",
    "        choices_df,\n",
    "        left_on=['pc4', 'sub_clean'],\n",
    "        right_on=['postcode', 'suburb_clean'],\n",
    "        how='left',\n",
    "    )\n",
    "    df[out_col] = df['location_id']\n",
    "\n",
    "    # fuzzy fallback ---------------------------------------------------------\n",
    "    misses = df[df[out_col].isna()].index\n",
    "    for pc4, idx in df.loc[misses].groupby('pc4').groups.items():\n",
    "        cand = choices_df.loc[choices_df.postcode == pc4, 'suburb_clean']\n",
    "        if cand.empty:\n",
    "            continue\n",
    "        choices = cand.tolist()\n",
    "        for i in idx:\n",
    "            target = df.at[i, 'sub_clean']\n",
    "            best, score, _ = process.extractOne(target, choices, scorer=fuzz.WRatio)\n",
    "            if score >= fuzz_thresh:\n",
    "                df.at[i, out_col] = choices_df.query(\n",
    "                    'postcode == @pc4 & suburb_clean == @best'\n",
    "                ).iloc[0]['location_id']\n",
    "\n",
    "    # Convert to nullable integer type to handle NaN properly\n",
    "    df[out_col] = df[out_col].astype('Int64')  # Capital I for nullable int\n",
    "    \n",
    "    return df.drop(columns=['pc4', 'sub_clean', 'postcode', 'suburb_clean', 'location_id'])\n",
    "\n",
    "# ------------------------------------------------------------------ 1) load cached location data (CSV)\n",
    "csv_path = Path('data/cario_location_data.csv')\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError('Expected CSV cache at data/cario_location_data.csv – run the scraper first.')\n",
    "\n",
    "location_data = {}\n",
    "for _, row in pd.read_csv(csv_path).iterrows():\n",
    "    try:\n",
    "        location_data[str(row['Postcode']).zfill(4)] = ast.literal_eval(row['Response'])\n",
    "    except Exception:\n",
    "        continue  # skip unparsable rows\n",
    "\n",
    "loc_df = _build_loc_df(location_data)\n",
    "\n",
    "# ------------------------------------------------------------------ 2) load & clean your input datasets\n",
    "aus = (\n",
    "    pd.read_csv('input/australian_postcodes_2025.csv')\n",
    "    .loc[lambda d: d.chargezone.notna(), ['postcode', 'locality', 'state', 'long', 'lat']]\n",
    "    .rename(\n",
    "        columns={\n",
    "            'postcode': 'Customer Postcode',\n",
    "            'locality': 'Customer Locality',\n",
    "            'state': 'Customer State',\n",
    "            'long': 'Customer Long',\n",
    "            'lat': 'Customer Lat',\n",
    "        }\n",
    "    )\n",
    ")\n",
    "aus['Customer Locality'] = aus['Customer Locality'].str.title()\n",
    "\n",
    "wh = (\n",
    "    pd.read_excel('input/parameters.xlsx', 'Warehouses')\n",
    "    .query(\"`Sending Warehouse`.isin(['Brisbane','Melbourne','Sydney','Perth'])\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            'Suburb':  'Warehouse Suburb',\n",
    "            'Address': 'Warehouse Address',\n",
    "            'Postcode':'Warehouse Postcode',\n",
    "            'State':   'Warehouse State',\n",
    "            'Long':    'Warehouse Long',\n",
    "            'Lat':     'Warehouse Lat',\n",
    "        }\n",
    "    )\n",
    "    .query(\"`Warehouse Suburb` != 'Welshpool'\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------ 3) map to/from IDs\n",
    "aus = assign_ids(aus, 'Customer Postcode',  'Customer Locality',  'to_id',   loc_df)\n",
    "wh  = assign_ids(wh,  'Warehouse Postcode', 'Warehouse Suburb',   'from_id', loc_df)\n",
    "\n",
    "# ------------------------------------------------------------------ 4) build unique combinations\n",
    "unique_combinations = (\n",
    "    aus.merge(\n",
    "        wh[['Warehouse Postcode', 'Warehouse Suburb', 'Warehouse State', 'from_id']],\n",
    "        how='cross',\n",
    "    )\n",
    "    .query('to_id.notnull() & from_id.notnull()')  # both IDs must be present\n",
    "    .drop_duplicates(subset=['Customer Postcode', 'Warehouse Postcode'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "# Testing\n",
    "unique_combinations = unique_combinations.sample(n=200, random_state=42)\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Fetching quotes for 200 lanes…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching quotes: 100%|██████████| 22/22 [00:40<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to data/price_to_hds_hand_unload_cario_20250903-135503.csv\n",
      "  • Success: 22/22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Async HTTP/2 Cario multi-account rate fetcher (httpx + uvloop + orjson) - 8 retries\n",
    "import asyncio\n",
    "import uvloop; uvloop.install()\n",
    "import httpx, orjson, pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from contextlib import AsyncExitStack\n",
    "import os\n",
    "import random\n",
    "\n",
    "MAX_RETRIES = 8\n",
    "SLEEP_BASE  = 0.5  # seconds\n",
    "\n",
    "# ---------- supply your creds and routing ----------\n",
    "# Example only - load from env or a secrets file in practice\n",
    "SESSION_TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1laWRlbnRpZmllciI6Ijk5MDkiLCJodHRwOi8vc2NoZW1hcy54bWxzb2FwLm9yZy93cy8yMDA1LzA1L2lkZW50aXR5L2NsYWltcy9uYW1lIjoiWl9HcmlsbHNfQXVzdHJhbGlhIiwiaHR0cDovL3NjaGVtYXMueG1sc29hcC5vcmcvd3MvMjAwNS8wNS9pZGVudGl0eS9jbGFpbXMvZW1haWxhZGRyZXNzIjoianVzdGluLm5nQHpncmlsbHMuY29tLmF1IiwiQXNwTmV0LklkZW50aXR5LlNlY3VyaXR5U3RhbXAiOiJNNVBVUFVZSUhYSVBYQ0ZSNVBDVjJWWVNMRFVUNFQ2QSIsImh0dHA6Ly9zY2hlbWFzLm1pY3Jvc29mdC5jb20vd3MvMjAwOC8wNi9pZGVudGl0eS9jbGFpbXMvcm9sZSI6IkRlc3BhdGNoUGFyZW50QWNjdCIsImh0dHA6Ly93d3cuYXNwbmV0Ym9pbGVycGxhdGUuY29tL2lkZW50aXR5L2NsYWltcy90ZW5hbnRJZCI6IjkiLCJDdXN0b21lcklEIjoiNzExNSIsInN1YiI6Ijk5MDkiLCJqdGkiOiJmZWZlM2ZlYi02M2Q5LTRmMDctODYyMi01NDRmOGFlYzgyOWUiLCJpYXQiOjE3NTY3OTQ5ODgsIm5iZiI6MTc1Njc5NDk4OCwiZXhwIjoxNzU2ODgxMzg4LCJpc3MiOiJGTVMiLCJhdWQiOiJGTVMifQ.dcHOlYolnRIbIsrrTif0rbV4XpjSSwCmhZdWqt1b3_w\"\n",
    "CREDENTIALS = {\n",
    "    # keys are arbitrary labels you choose\n",
    "    \"MEL\": {\"customer_id\": 16071, \"auth_token\": SESSION_TOKEN, \"tenant_id\": \"\"},\n",
    "    \"SYD\": {\"customer_id\": 16070, \"auth_token\": SESSION_TOKEN, \"tenant_id\": \"\"},\n",
    "    \"BNE\": {\"customer_id\": 16072, \"auth_token\": SESSION_TOKEN, \"tenant_id\": \"\"},\n",
    "}\n",
    "\n",
    "# Map each origin suburb in your DataFrame to an account key above\n",
    "WH_TO_ACCOUNT = {\n",
    "    \"Epping\": \"SYD\",\n",
    "    \"Chipping Norton\": \"SYD\",\n",
    "    \"Berrinba\":  \"BNE\",\n",
    "}\n",
    "\n",
    "BASE_URL  = \"https://integrate.cario.com.au/api/\"\n",
    "COUNTRY   = {\"id\": 36, \"iso2\":\"AU\", \"iso3\":\"AUS\", \"name\":\"AUSTRALIA\"}\n",
    "\n",
    "def _headers(cred: dict) -> dict:\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {cred['auth_token']}\",\n",
    "        \"CustomerId\": str(cred[\"customer_id\"]),\n",
    "        \"TenantId\": cred.get(\"tenant_id\", \"\"),\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def get_next_weekday(today: datetime) -> str:\n",
    "    wd = today.weekday()\n",
    "    if wd in (5, 6):  # Sat or Sun -> next Monday\n",
    "        return (today + timedelta(days=(7 - wd))).date().isoformat()\n",
    "    return today.date().isoformat()\n",
    "\n",
    "def build_payload(row: pd.Series, dims: list, customer_id: int) -> dict:\n",
    "    L, W, H, weight, qty = dims\n",
    "    return {\n",
    "        \"customerId\":   customer_id,\n",
    "        \"pickupDate\":   get_next_weekday(datetime.now()),\n",
    "        \"pickupAddress\": {\n",
    "            \"name\": \"Quote\", \"line1\": \"Quote\",\n",
    "            \"location\": {\n",
    "                \"id\": int(row[\"from_id\"]),\n",
    "                \"locality\": row[\"Warehouse Suburb\"],\n",
    "                \"state\":    row[\"Warehouse State\"],\n",
    "                \"postcode\": str(row[\"Warehouse Postcode\"]),\n",
    "                \"country\":  COUNTRY\n",
    "            }\n",
    "        },\n",
    "        \"deliveryAddress\": {\n",
    "            \"name\": \"Quote\", \"line1\": \"Quote\",\n",
    "            \"location\": {\n",
    "                \"id\": int(row[\"to_id\"]),\n",
    "                \"locality\": row[\"Customer Locality\"],\n",
    "                \"state\":    row[\"Customer State\"],\n",
    "                \"postcode\": str(row[\"Customer Postcode\"]),\n",
    "                \"country\":  COUNTRY\n",
    "            },\n",
    "            \"isResidential\": True\n",
    "        },\n",
    "        \"totalItems\":  qty,\n",
    "        \"totalWeight\": weight,\n",
    "        \"transportUnits\": [{\n",
    "            \"transportUnitType\": \"Pallet\",\n",
    "            \"quantity\": qty,\n",
    "            \"length\":   L,\n",
    "            \"width\":    W,\n",
    "            \"height\":   H,\n",
    "            \"weight\":   weight,\n",
    "            \"volume\":   (L * W * H) / 1_000_000\n",
    "        }],\n",
    "        \"optionHandUnload\": True\n",
    "    }\n",
    "\n",
    "async def fetch_quote(client: httpx.AsyncClient, customer_id: int, row: pd.Series,\n",
    "                      dims: list, sem: asyncio.Semaphore, row_index: int) -> tuple[int, dict]:\n",
    "    async with sem:\n",
    "        payload = build_payload(row, dims, customer_id)\n",
    "        content = orjson.dumps(payload)\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                r = await client.post(\"/Consignment/GetQuotes\", content=content)\n",
    "                if r.status_code == 200:\n",
    "                    return row_index, r.json()\n",
    "                # Respect simple rate limit hints\n",
    "                if r.status_code in (429, 503):\n",
    "                    retry_after = r.headers.get(\"Retry-After\")\n",
    "                    if retry_after:\n",
    "                        try:\n",
    "                            await asyncio.sleep(float(retry_after))\n",
    "                        except Exception:\n",
    "                            await asyncio.sleep(SLEEP_BASE * (attempt + 1))\n",
    "                    else:\n",
    "                        # jittered backoff\n",
    "                        await asyncio.sleep(SLEEP_BASE * (attempt + 1) * random.uniform(0.8, 1.3))\n",
    "                    continue\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    return row_index, {\"error\": r.status_code, \"msg\": r.text[:500]}\n",
    "            except httpx.TimeoutException:\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    return row_index, {\"error\": \"timeout\"}\n",
    "            except Exception as e:\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    return row_index, {\"error\": \"exception\", \"msg\": str(e)}\n",
    "            await asyncio.sleep(SLEEP_BASE * (attempt + 1) * random.uniform(0.8, 1.3))\n",
    "\n",
    "async def fetch_all_multi(df: pd.DataFrame, dims: list,\n",
    "                          creds: dict, wh_to_account: dict,\n",
    "                          conc_per_account: int = 16, req_timeout: float = 60.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"Response\"] = \"\"\n",
    "\n",
    "    # Build one client and one semaphore per account\n",
    "    async with AsyncExitStack() as stack:\n",
    "        clients = {}\n",
    "        sems    = {}\n",
    "        for key, cred in creds.items():\n",
    "            clients[key] = await stack.enter_async_context(\n",
    "                httpx.AsyncClient(\n",
    "                    base_url=BASE_URL,\n",
    "                    headers=_headers(cred),\n",
    "                    http2=True,\n",
    "                    limits=httpx.Limits(max_keepalive_connections=1, max_connections=conc_per_account),\n",
    "                    timeout=httpx.Timeout(req_timeout),\n",
    "                )\n",
    "            )\n",
    "            sems[key] = asyncio.Semaphore(conc_per_account)\n",
    "\n",
    "        tasks = []\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            wh_suburb = str(row[\"Warehouse Suburb\"])\n",
    "            acct_key  = wh_to_account.get(wh_suburb)\n",
    "            if acct_key is None:\n",
    "                # Skip or raise. Here we mark an error row.\n",
    "                df.at[i, \"Response\"] = orjson.dumps({\"error\": \"missing_account_mapping\", \"warehouse\": wh_suburb}).decode()\n",
    "                continue\n",
    "\n",
    "            client      = clients[acct_key]\n",
    "            sem         = sems[acct_key]\n",
    "            customer_id = creds[acct_key][\"customer_id\"]\n",
    "\n",
    "            tasks.append(asyncio.create_task(\n",
    "                fetch_quote(client, customer_id, row, dims, sem, i)\n",
    "            ))\n",
    "\n",
    "        for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Fetching quotes\"):\n",
    "            idx, res = await fut\n",
    "            df.at[idx, \"Response\"] = orjson.dumps(res).decode()\n",
    "\n",
    "    return df\n",
    "\n",
    "def price_quote_cario_multi(df: pd.DataFrame, dims: list,\n",
    "                            creds: dict, wh_to_account: dict,\n",
    "                            conc_per_account: int = 16) -> pd.DataFrame:\n",
    "    try:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return asyncio.get_event_loop().run_until_complete(\n",
    "        fetch_all_multi(df, dims, creds, wh_to_account, conc_per_account=conc_per_account)\n",
    "    )\n",
    "\n",
    "# ---------- usage ----------\n",
    "# example: route all lanes in your current DataFrame\n",
    "def filter_eastern_states_with_postcode(df, include_pc=3168, n_others=20, random_state=42):\n",
    "    # filter to VIC, NSW, QLD customers\n",
    "    east = df[df['Customer State'].isin(['VIC','NSW','QLD'])]\n",
    "    \n",
    "    # choose postcodes: force include_pc + n_others random others from filtered set\n",
    "    others = east.loc[east['Customer Postcode'] != include_pc, 'Customer Postcode'].drop_duplicates()\n",
    "    selected = {include_pc} | set(others.sample(n=min(n_others, len(others)), random_state=random_state))\n",
    "    \n",
    "    # get all rows for selected postcodes\n",
    "    relevant_combo = east[east['Customer Postcode'].isin(selected)].reset_index(drop=True)\n",
    "    return relevant_combo\n",
    "\n",
    "relevant_lanes = filter_eastern_states_with_postcode(unique_combinations)\n",
    "\n",
    "# ---------- usage ----------\n",
    "if request_rates:\n",
    "\n",
    "    # unique_combinations = filter_eastern_states_with_postcode(unique_combinations)\n",
    "    print(f\"⏳ Fetching quotes for {len(unique_combinations)} lanes…\")\n",
    "\n",
    "    out = price_quote_cario_multi(relevant_lanes, DIMENSIONS, CREDENTIALS, WH_TO_ACCOUNT, conc_per_account=32)\n",
    "    ts  = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fn = f\"data/{prefix}{ts}.csv\"\n",
    "    out.to_csv(fn, index=False)\n",
    "    print(f\"✅ Saved to {fn}\")\n",
    "    good = out[\"Response\"].apply(lambda x: \"error\" not in orjson.loads(x)).sum()\n",
    "    print(f\"  • Success: {good}/{len(out)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from datetime import datetime\n",
    "\n",
    "def get_latest_input_file(prefix: str) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    Scan the `data/` folder for files named like\n",
    "      {prefix}YYYYMMDD-HHMMSS.csv\n",
    "    and return both the filename and the timestamp string.\n",
    "    \"\"\"\n",
    "    latest_ts: datetime | None = None\n",
    "    latest_file: str | None = None\n",
    "    # Pre‐compile regex for speed\n",
    "    pattern = re.compile(rf'^{re.escape(prefix)}(\\d{{8}}-\\d{{6}})\\.csv$')\n",
    "\n",
    "    for fname in os.listdir('data/'):\n",
    "        m = pattern.match(fname)\n",
    "        if not m:\n",
    "            continue\n",
    "        ts_str = m.group(1)\n",
    "        try:\n",
    "            ts = datetime.strptime(ts_str, '%Y%m%d-%H%M%S')\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if latest_ts is None or ts > latest_ts:\n",
    "            latest_ts = ts\n",
    "            latest_file = fname\n",
    "\n",
    "    if latest_file is None:\n",
    "        return None, None\n",
    "\n",
    "    # Return both filename and the extracted timestamp\n",
    "    return latest_file, latest_ts.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "file, timestamp = get_latest_input_file(prefix)\n",
    "print(f\"Latest file: {file}, Timestamp: {timestamp}\")\n",
    "\n",
    "# Get the latest file and its timestamp\n",
    "latest_file, latest_ts = get_latest_input_file(prefix)\n",
    "\n",
    "if latest_file:\n",
    "    # Read it into a DataFrame\n",
    "    data = pd.read_csv(os.path.join('data', latest_file))\n",
    "    print(f\"Loaded '{latest_file}' (timestamp: {latest_ts}) — shape: {data.shape}\")\n",
    "else:\n",
    "    data = pd.DataFrame()\n",
    "    print(\"No matching file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_best(row: pd.Series, scrape_ts: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    From row['Response'] (a JSON list of quotes), pick the cheapest one\n",
    "    and return price, transit_days, freight_company & carrier_id.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        routes = json.loads(row.get('Response','[]'))\n",
    "    except json.JSONDecodeError:\n",
    "        routes = []\n",
    "    if not isinstance(routes, list) or not routes:\n",
    "        return pd.Series({\n",
    "            'cheapest_price':   None,\n",
    "            'transit_time':     None,\n",
    "            'freight_company':  None,\n",
    "            'carrier_id':       None\n",
    "        })\n",
    "\n",
    "    # find the cheapest by total\n",
    "    best = min(routes, key=lambda r: r.get('total', float('inf')))\n",
    "    price   = best.get('total')\n",
    "    company = best.get('carrierName')\n",
    "    cid     = best.get('carrierId')\n",
    "\n",
    "    # compute transit days\n",
    "    eta = best.get('eta')\n",
    "    if eta:\n",
    "        try:\n",
    "            eta_dt   = datetime.fromisoformat(eta)\n",
    "            base_dt  = datetime.strptime(scrape_ts, '%Y%m%d-%H%M%S')\n",
    "            days     = (eta_dt - base_dt).days\n",
    "        except Exception:\n",
    "            days = None\n",
    "    else:\n",
    "        days = None\n",
    "\n",
    "    return pd.Series({\n",
    "        'cheapest_price':   price,\n",
    "        'transit_time':     days,\n",
    "        'freight_company':  company,\n",
    "        'carrier_id':       cid\n",
    "    })\n",
    "\n",
    "# extract cheapest & transit\n",
    "out = data.join(\n",
    "    data.apply(lambda r: extract_best(r, latest_ts), axis=1)\n",
    ")\n",
    "\n",
    "out.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_cheapest_per_postcode_pair(data: pd.DataFrame, scrape_ts: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the absolute cheapest freight rate for each unique \n",
    "    (Customer Postcode, Warehouse Postcode) pair.\n",
    "\n",
    "    Applies special rule: If freight_company is 'Jet Couriers' and transit time is missing,\n",
    "    defaults transit_time_days to 1.\n",
    "\n",
    "    Returns DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        # Parse quotes from Response\n",
    "        try:\n",
    "            quotes = json.loads(row['Response'])\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue\n",
    "            \n",
    "        if not isinstance(quotes, list) or not quotes:\n",
    "            continue\n",
    "            \n",
    "        # Find cheapest quote\n",
    "        best_quote = min(quotes, key=lambda q: q.get('total', float('inf')))\n",
    "        \n",
    "        # Extract fields\n",
    "        price = best_quote.get('total')\n",
    "        carrier_id = best_quote.get('carrierId')\n",
    "        company = best_quote.get('carrierName')\n",
    "        \n",
    "        # Calculate transit time\n",
    "        transit_days = None\n",
    "        if eta := best_quote.get('eta'):\n",
    "            try:\n",
    "                eta_dt = datetime.fromisoformat(eta)\n",
    "                base_dt = datetime.strptime(scrape_ts, '%Y%m%d-%H%M%S')\n",
    "                transit_days = (eta_dt - base_dt).days\n",
    "            except Exception:\n",
    "                pass  # leave as None if parsing fails\n",
    "        \n",
    "        # Special rule: Jet Couriers → default to 1-day transit if missing\n",
    "        if company == 'Jet Couriers' and transit_days is None:\n",
    "            transit_days = 1\n",
    "        \n",
    "        # Collect result\n",
    "        results.append({\n",
    "            'customer_postcode': row['Customer Postcode'],\n",
    "            'customer_locality': row['Customer Locality'],\n",
    "            'warehouse_postcode': row['Warehouse Postcode'],\n",
    "            'warehouse_locality': row['Warehouse Suburb'],\n",
    "            'freight_price': price,\n",
    "            'transit_time_days': transit_days,\n",
    "            'carrier_id': carrier_id,\n",
    "            'freight_company': company\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            'customer_postcode', 'customer_locality', 'warehouse_postcode',\n",
    "            'warehouse_locality', 'freight_price', 'transit_time_days',\n",
    "            'carrier_id', 'freight_company'\n",
    "        ])\n",
    "    \n",
    "    # Sort by price so cheapest is first within each group\n",
    "    df = df.sort_values('freight_price')\n",
    "    \n",
    "    # Keep only the cheapest quote per (customer_postcode, warehouse_postcode)\n",
    "    final = df.drop_duplicates(\n",
    "        subset=['customer_postcode', 'warehouse_postcode'],\n",
    "        keep='first'\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Convert numeric fields\n",
    "    final['freight_price'] = pd.to_numeric(final['freight_price'], errors='coerce')\n",
    "    final['transit_time_days'] = pd.to_numeric(final['transit_time_days'], errors='coerce')\n",
    "    final['carrier_id'] = pd.to_numeric(final['carrier_id'], errors='coerce')\n",
    "    \n",
    "    # Final column order\n",
    "    final = final[[\n",
    "        'customer_postcode',\n",
    "        'customer_locality',\n",
    "        'warehouse_postcode',\n",
    "        'warehouse_locality',\n",
    "        'freight_price',\n",
    "        'transit_time_days',\n",
    "        'carrier_id',\n",
    "        'freight_company'\n",
    "    ]]\n",
    "    \n",
    "    return final\n",
    "\n",
    "# Usage example:\n",
    "cheapest_price_lane = get_cheapest_per_postcode_pair(data, latest_ts)\n",
    "\n",
    "# Get Cheapest for Each Postcode\n",
    "def get_cheapest_per_postcode(df):\n",
    "    \"\"\"\n",
    "    Get the cheapest freight rate for each customer postcode across all warehouses.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns including customer_postcode, freight_price\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with cheapest rate per postcode\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Group by customer postcode and find the row with minimum freight price\n",
    "    cheapest_per_postcode = df.loc[df.groupby('customer_postcode')['freight_price'].idxmin()].reset_index(drop=True)\n",
    "    \n",
    "    return cheapest_per_postcode\n",
    "\n",
    "# Get cheapest rate for each customer postcode\n",
    "cheapest_per_postcode = get_cheapest_per_postcode(cheapest_price_lane)\n",
    "print(f\"Cheapest rates for {len(cheapest_per_postcode)} unique postcodes:\")\n",
    "cheapest_per_postcode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheapest_price_lane[cheapest_price_lane['customer_postcode'] == 3168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hds_rates_map(hds_data, warehouse_data=None):\n",
    "    \"\"\"\n",
    "    Create a choropleth map showing HDS rates from Cario freight quotes.\n",
    "    \n",
    "    Parameters:\n",
    "    hds_data: DataFrame with columns ['customer_postcode', 'customer_locality', 'warehouse_postcode', \n",
    "                                    'warehouse_locality', 'freight_price', 'transit_time_days', \n",
    "                                    'carrier_id', 'freight_company']\n",
    "    warehouse_data: Optional DataFrame with warehouse location data for markers\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    import folium\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Make a copy of the data to avoid modifying the original DataFrame\n",
    "    data = hds_data.copy()\n",
    "    \n",
    "    # Ensure that the price fields are numeric\n",
    "    data['freight_price'] = pd.to_numeric(data['freight_price'], errors='coerce')\n",
    "    data['transit_time_days'] = pd.to_numeric(data['transit_time_days'], errors='coerce')\n",
    "\n",
    "    # Clip freight prices at $400 maximum\n",
    "    data['freight_price'] = data['freight_price'].clip(upper=400)\n",
    "    \n",
    "    # Format the postcode field (and ensure string type)\n",
    "    data['customer_postcode'] = data['customer_postcode'].astype(str).str.zfill(4)\n",
    "    \n",
    "    # Load and prepare the boundary GeoDataFrame\n",
    "    boundary = gpd.read_file(\"boundary.json\")\n",
    "    boundary['POA_NAME'] = boundary['POA_NAME'].str[:4]\n",
    "    boundary = boundary[boundary['geometry'].notnull()]\n",
    "    \n",
    "    # Merge boundary with the data based on the postcode\n",
    "    hds_geometry_df = boundary.merge(data, left_on='POA_NAME', right_on='customer_postcode')\n",
    "    \n",
    "    # Create a base map centered over Australia\n",
    "    hds_rate_map = folium.Map(location=[-24.15, 133.25], zoom_start=3)\n",
    "    \n",
    "    # Add a Choropleth layer for HDS Freight Price\n",
    "    price_choropleth = folium.Choropleth(\n",
    "        geo_data=boundary,\n",
    "        data=data,\n",
    "        bins=8,\n",
    "        columns=[\"customer_postcode\", \"freight_price\"],\n",
    "        key_on=\"feature.properties.POA_NAME\",\n",
    "        fill_color=\"OrRd\",\n",
    "        nan_fill_color=\"lightgray\",\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        name='Freight Price',\n",
    "        highlight=True,\n",
    "        legend_name=\"Freight Price ($AUD)\",\n",
    "    ).add_to(hds_rate_map)\n",
    "    \n",
    "    # Add a Choropleth layer for Transit Time\n",
    "    transit_choropleth = folium.Choropleth(\n",
    "        geo_data=boundary,\n",
    "        data=data,\n",
    "        bins=6,\n",
    "        columns=[\"customer_postcode\", \"transit_time_days\"],\n",
    "        key_on=\"feature.properties.POA_NAME\",\n",
    "        fill_color=\"YlGnBu\",\n",
    "        nan_fill_color=\"white\",\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        name='Transit Time',\n",
    "        highlight=True,\n",
    "        legend_name=\"Transit Time (Days)\"\n",
    "    ).add_to(hds_rate_map)\n",
    "    \n",
    "    # Define style and highlight functions for GeoJson layer\n",
    "    style_function = lambda x: {\n",
    "        'fillColor': '#ffffff',\n",
    "        'color': '#000000',\n",
    "        'fillOpacity': 0.01,\n",
    "        'weight': 0.01\n",
    "    }\n",
    "    highlight_function = lambda x: {\n",
    "        'fillColor': '#ffffff',\n",
    "        'color': '#000000',\n",
    "        'fillOpacity': 0.01,\n",
    "        'weight': 0.01\n",
    "    }\n",
    "    \n",
    "    # Add a GeoJson layer for detailed HDS info with tooltip\n",
    "    hds_info_layer = folium.features.GeoJson(\n",
    "        hds_geometry_df,\n",
    "        style_function=style_function,\n",
    "        control=True,\n",
    "        name='HDS Details',\n",
    "        highlight_function=highlight_function,\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=[\"customer_postcode\", \"customer_locality\", \"freight_price\", \n",
    "                   \"transit_time_days\", \"freight_company\", \"warehouse_locality\"],\n",
    "            aliases=[\"Postcode: \", \"Locality: \", \"Price: $\", \n",
    "                    \"Transit Days: \", \"Carrier: \", \"From Warehouse: \"],\n",
    "            style=(\"background-color: white; color: #333333; font-family: arial; \"\n",
    "                   \"font-size: 12px; padding: 10px;\")\n",
    "        )\n",
    "    )\n",
    "    hds_rate_map.add_child(hds_info_layer)\n",
    "    \n",
    "    # Add warehouse markers if warehouse data is provided\n",
    "    if warehouse_data is not None:\n",
    "        wh_data = warehouse_data.copy()\n",
    "        \n",
    "        # Group warehouses by price ranges for different marker colors\n",
    "        if 'freight_price' in wh_data.columns:\n",
    "            wh_data['freight_price'] = pd.to_numeric(wh_data['freight_price'], errors='coerce')\n",
    "            \n",
    "            # Warehouses with cheap rates (under $150)\n",
    "            wh_cheap = wh_data[wh_data[\"freight_price\"] < 150].copy().reset_index(drop=True)\n",
    "            cheap_group = folium.FeatureGroup(name='Warehouses < $150')\n",
    "            for i in range(len(wh_cheap)):\n",
    "                if pd.notna(wh_cheap.iloc[i].get('warehouse_lat')) and pd.notna(wh_cheap.iloc[i].get('warehouse_long')):\n",
    "                    cheap_group.add_child(folium.Marker(\n",
    "                        location=[wh_cheap.iloc[i]['warehouse_lat'], wh_cheap.iloc[i]['warehouse_long']],\n",
    "                        icon=folium.Icon(color='green', icon='info-sign'),\n",
    "                        tooltip=f\"{wh_cheap.iloc[i].get('warehouse_locality', 'Unknown')} | ${wh_cheap.iloc[i].get('freight_price', 'N/A')}\"\n",
    "                    ))\n",
    "            \n",
    "            # Warehouses with expensive rates ($150+)\n",
    "            wh_expensive = wh_data[wh_data[\"freight_price\"] >= 150].copy().reset_index(drop=True)\n",
    "            expensive_group = folium.FeatureGroup(name='Warehouses ≥ $150')\n",
    "            for i in range(len(wh_expensive)):\n",
    "                if pd.notna(wh_expensive.iloc[i].get('warehouse_lat')) and pd.notna(wh_expensive.iloc[i].get('warehouse_long')):\n",
    "                    expensive_group.add_child(folium.Marker(\n",
    "                        location=[wh_expensive.iloc[i]['warehouse_lat'], wh_expensive.iloc[i]['warehouse_long']],\n",
    "                        icon=folium.Icon(color='red', icon='info-sign'),\n",
    "                        tooltip=f\"{wh_expensive.iloc[i].get('warehouse_locality', 'Unknown')} | ${wh_expensive.iloc[i].get('freight_price', 'N/A')}\"\n",
    "                    ))\n",
    "            \n",
    "            hds_rate_map.add_child(cheap_group)\n",
    "            hds_rate_map.add_child(expensive_group)\n",
    "        else:\n",
    "            # Add generic warehouse markers if no price data\n",
    "            wh_group = folium.FeatureGroup(name='Warehouses')\n",
    "            for i, row in wh_data.iterrows():\n",
    "                if pd.notna(row.get('warehouse_lat')) and pd.notna(row.get('warehouse_long')):\n",
    "                    wh_group.add_child(folium.Marker(\n",
    "                        location=[row['warehouse_lat'], row['warehouse_long']],\n",
    "                        icon=folium.Icon(color='blue', icon='info-sign'),\n",
    "                        tooltip=f\"{row.get('warehouse_locality', 'Unknown Warehouse')}\"\n",
    "                    ))\n",
    "            hds_rate_map.add_child(wh_group)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(hds_rate_map)\n",
    "    \n",
    "    return hds_rate_map\n",
    "\n",
    "# Usage example:\n",
    "hds_map = plot_hds_rates_map(cheapest_per_postcode)\n",
    "hds_map\n",
    "# hds_map.save('hds_rates_map.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
